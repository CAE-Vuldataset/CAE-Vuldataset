{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecurityPatchIdentificationRNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/shuwang127/SecurityPatchIdentificationRNN/blob/master/SecurityPatchIdentificationRNN.ipynb",
      "authorship_tag": "ABX9TyN+l1lpij6xV0RpcYZavOPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/SecurityPatchIdentificationRNN/blob/master/SecurityPatchIdentificationRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fko-ZzuoPWWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b25f67e-5817-471a-b370-022800e15101"
      },
      "source": [
        "'''\n",
        "    SecurityPatchIdentificationRNN: Security Patch Identification using RNN model.\n",
        "    Developer: Shu Wang\n",
        "    Date: 2020-08-08\n",
        "    Version: S2020.08.08-V4\n",
        "    Description: patch identification using both commit messages and normalized diff code.\n",
        "    File Structure:\n",
        "    SecurityPatchIdentificationRNN\n",
        "        |-- analysis                                # task analysis.\n",
        "        |-- data                                    # data storage.\n",
        "                |-- negatives                           # negative samples.\n",
        "                |-- positives                           # positive samples.\n",
        "                |-- security_patch                      # positive samples. (official)\n",
        "        |-- temp                                    # temporary stored variables.\n",
        "                |-- data.npy                            # raw data. (important)\n",
        "                |-- props.npy                           # properties of diff code. (important)\n",
        "                |-- msgs.npy                            # commit messages. (important)\n",
        "                |-- ...                                 # other temporary files. (trivial)\n",
        "        |-- SecurityPatchIdentificationRNN.ipynb    # main entrance. (Google Colaboratory)\n",
        "        |-- SecurityPatchIdentificationRNN.py       # main entrance. (Local)\n",
        "    Usage:\n",
        "        python SecurityPatchIdentificationRNN.py\n",
        "    Dependencies:\n",
        "        clang >= 6.0.0.2\n",
        "        torch >= 1.2.0+cu92\n",
        "        nltk  >= 3.3\n",
        "'''\n",
        "\n",
        "# dependencies.\n",
        "import os\n",
        "os.system('pip install clang')\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import clang.cindex\n",
        "import clang.enumerations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# environment settings.\n",
        "_COLAB_ = 0 if (os.getenv('COLAB_GPU', 'NONE') == 'NONE') else 1 # 0 : Local environment, 1 : Google Colaboratory.\n",
        "# file paths.\n",
        "rootPath = './drive/My Drive/Colab Notebooks/' if (_COLAB_) else './'\n",
        "dataPath = rootPath + '/data/'\n",
        "sDatPath = dataPath + '/security_patch/'\n",
        "pDatPath = dataPath + '/positives/'\n",
        "nDatPath = dataPath + '/negatives/'\n",
        "tempPath = rootPath + '/temp/'\n",
        "\n",
        "# hyper-parameters. (affect GPU memory size)\n",
        "_DiffEmbedDim_  = 128       # 128\n",
        "_DiffMaxLen_    = 600       # 200(0.7), 314(0.8), 609(0.9), 1100(0.95), 2200(0.98), 3289(0.99), 5000(0.995), 10000(0.9997)\n",
        "_DRnnHidSiz_    = 16        # 16\n",
        "_MsgEmbedDim_   = 128       # 128\n",
        "_MsgMaxLen_     = 200       # 54(0.9), 78(0.95), 130(0.98), 187(0.99), 268(0.995), 356(0.998), 516(0.999), 1434(1)\n",
        "_MRnnHidSiz_    = 32        # 16\n",
        "_TwinEmbedDim_  = 128       # 128\n",
        "_TwinMaxLen_    = 800       # 224(0.8), 425(0.9), 755(0.95), 1448(0.98), 2270(0.99)\n",
        "_TRnnHidSiz_    = 32        # 16\n",
        "# hyper-parameters. (affect training speed)\n",
        "_DRnnBatchSz_   = 128       # 128\n",
        "_DRnnLearnRt_   = 0.0001    # 0.0001\n",
        "_MRnnBatchSz_   = 128       # 128\n",
        "_MRnnLearnRt_   = 0.0001    # 0.0001\n",
        "_PRnnBatchSz_   = 256       # 256\n",
        "_PRnnLearnRt_   = 0.0005    # 0.0005\n",
        "_TRnnBatchSz_   = 256       # 256\n",
        "_TRnnLearnRt_   = 0.0005    # 0.0005\n",
        "# hyper-parameters. (trivial network parameters, unnecessary to modify)\n",
        "_DiffExtraDim_  = 2         # 2\n",
        "_TwinExtraDim_  = 1         # 1\n",
        "_DRnnHidLay_    = 1         # 1\n",
        "_MRnnHidLay_    = 1         # 1\n",
        "_TRnnHidLay_    = 1         # 1\n",
        "# hyper-parameters. (epoch related parameters, unnecessary to modify)\n",
        "_DRnnMaxEpoch_  = 1000      # 1000\n",
        "_DRnnPerEpoch_  = 1         # 1\n",
        "_DRnnJudEpoch_  = 10        # 10\n",
        "_MRnnMaxEpoch_  = 1000      # 1000\n",
        "_MRnnPerEpoch_  = 1         # 1\n",
        "_MRnnJudEpoch_  = 10        # 10\n",
        "_PRnnMaxEpoch_  = 1000      # 1000\n",
        "_PRnnPerEpoch_  = 1         # 1\n",
        "_PRnnJudEpoch_  = 10        # 10\n",
        "_TRnnMaxEpoch_  = 1000      # 1000\n",
        "_TRnnPerEpoch_  = 1         # 1\n",
        "_TRnnJudEpoch_  = 10        # 10\n",
        "# hyper-parameters. (flow control)\n",
        "_DEBUG_ = 0 #  0 : release\n",
        "            #  1 : debug\n",
        "_LOCK_  = 0 #  0 : unlocked - create random split sets.\n",
        "            #  1 : locked   - use the saved split sets.\n",
        "_MODEL_ = 0 #  0 : unlocked - train a new model.\n",
        "            #  1 : locked   - load the saved model.\n",
        "_DTYP_  = 1 #  0 : maintain both diff code and context code.\n",
        "            #  1 : only maintain diff code.\n",
        "_CTYP_  = 0 #  0 : maintain both the code and comments.\n",
        "            #  1 : only maintain code and delete comments.\n",
        "_NIND_ =  1 # -1 : not abstract tokens. (and will disable _NLIT_)\n",
        "            #  0 : abstract identifiers with VAR/FUNC.\n",
        "            #  1 : abstract identifiers with VARn/FUNCn.\n",
        "_NLIT_  = 1 #  0 : abstract literals with LITERAL.\n",
        "            #  1 : abstract literals with LITERAL/n.\n",
        "_TWIN_  = 1 #  0 : only twin neural network.\n",
        "            #  1 : twins + msg neural network.\n",
        "\n",
        "# print setting.\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "def demoDiffRNN():\n",
        "    '''\n",
        "    demo program of using diff code to identify patches.\n",
        "    '''\n",
        "\n",
        "    # load data.\n",
        "    if (not os.path.exists(tempPath + '/data.npy')): # | (not _DEBUG_)\n",
        "        dataLoaded = ReadData()\n",
        "    else:\n",
        "        dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n",
        "\n",
        "    # get the diff file properties.\n",
        "    if (not os.path.exists(tempPath + '/props.npy')):\n",
        "        diffProps = GetDiffProps(dataLoaded)\n",
        "    else:\n",
        "        diffProps = np.load(tempPath + '/props.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Load ' + str(len(diffProps)) + ' diff property data from ' + tempPath + '/props.npy.')\n",
        "\n",
        "    # only maintain the diff parts of the code.\n",
        "    #diffProps = ProcessTokens(diffProps, dType=_DTYP_, cType=_CTYP_)\n",
        "    # normalize the tokens of identifiers, literals, and comments.\n",
        "    #diffProps = AbstractTokens(diffProps, iType=_NIND_, lType=_NLIT_)\n",
        "    # get the diff token vocabulary.\n",
        "    diffVocab, diffMaxLen = GetDiffVocab(diffProps)\n",
        "    # get the max diff length.\n",
        "    diffMaxLen = _DiffMaxLen_ if (diffMaxLen > _DiffMaxLen_) else diffMaxLen\n",
        "    # get the diff token dictionary.\n",
        "    diffDict = GetDiffDict(diffVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    diffPreWeights = GetDiffEmbed(diffDict, _DiffEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    diffData, diffLabels = GetDiffMapping(diffProps, diffMaxLen, diffDict)\n",
        "    # change the tokentypes into one-hot vector.\n",
        "    diffData = UpdateTokenTypes(diffData)\n",
        "\n",
        "    # split data into rest/test dataset.\n",
        "    dataRest, labelRest, dataTest, labelTest = SplitData(diffData, diffLabels, 'test', rate=0.2)\n",
        "    # split data into train/valid dataset.\n",
        "    dataTrain, labelTrain, dataValid, labelValid = SplitData(dataRest, labelRest, 'valid', rate=0.2)\n",
        "    print('[INFO] <main> Get ' + str(len(dataTrain)) + ' TRAIN data, ' + str(len(dataValid)) + ' VALID data, '\n",
        "          + str(len(dataTest)) + ' TEST data. (Total: ' + str(len(dataTrain)+len(dataValid)+len(dataTest)) + ')')\n",
        "\n",
        "    # DiffRNNTrain\n",
        "    if (_MODEL_) & (os.path.exists(tempPath + '/model_DiffRNN.pth')):\n",
        "        preWeights = torch.from_numpy(diffPreWeights)\n",
        "        model = DiffRNN(preWeights, hiddenSize=_DRnnHidSiz_, hiddenLayers=_DRnnHidLay_)\n",
        "        model.load_state_dict(torch.load(tempPath + '/model_DiffRNN.pth'))\n",
        "    else:\n",
        "        model = DiffRNNTrain(dataTrain, labelTrain, dataValid, labelValid, preWeights=diffPreWeights,\n",
        "                             batchsize=_DRnnBatchSz_, learnRate=_DRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n",
        "\n",
        "    # DiffRNNTest\n",
        "    predictions, accuracy = DiffRNNTest(model, dataTest, labelTest, batchsize=_DRnnBatchSz_)\n",
        "    _, confusion = OutputEval(predictions, labelTest, 'DiffRNN')\n",
        "\n",
        "    return\n",
        "\n",
        "def ReadData():\n",
        "    '''\n",
        "    Read data from the files.\n",
        "    :return: data - a set of commit message, diff code, and labels.\n",
        "    [[['', ...], [['', ...], ['', ...], ...], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def ReadCommitMsg(filename):\n",
        "        '''\n",
        "        Read commit message from a file.\n",
        "        :param filename: file name (string).\n",
        "        :return: commitMsg - commit message.\n",
        "        ['line', 'line', ...]\n",
        "        '''\n",
        "\n",
        "        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n",
        "        lines = fp.readlines()  # read all lines.\n",
        "        #numLines = len(lines)   # get the line number.\n",
        "        #print(lines)\n",
        "\n",
        "        # initialize commit message.\n",
        "        commitMsg = []\n",
        "        # get the wide range of commit message.\n",
        "        for line in lines:\n",
        "            if line.startswith('diff --git'):\n",
        "                break\n",
        "            else:\n",
        "                commitMsg.append(line)\n",
        "        #print(commitMsg)\n",
        "        # process the head of commit message.\n",
        "        while (1):\n",
        "            headMsg = commitMsg[0]\n",
        "            if (headMsg.startswith('From') or headMsg.startswith('Date:') or headMsg.startswith('Subject:')\n",
        "                    or headMsg.startswith('commit') or headMsg.startswith('Author:')):\n",
        "                commitMsg.pop(0)\n",
        "            else:\n",
        "                break\n",
        "        #print(commitMsg)\n",
        "        # process the tail of commit message.\n",
        "        dashLines = [i for i in range(len(commitMsg))\n",
        "                     if commitMsg[i].startswith('---')]  # finds all lines start with ---.\n",
        "        if (len(dashLines)):\n",
        "            lnum = dashLines[-1]  # last line number of ---\n",
        "            marks = [1 if (' file changed, ' in commitMsg[i] or ' files changed, ' in commitMsg[i]) else 0\n",
        "                     for i in range(lnum, len(commitMsg))]\n",
        "            if (sum(marks)):\n",
        "                for i in reversed(range(lnum, len(commitMsg))):\n",
        "                    commitMsg.pop(i)\n",
        "        #print(commitMsg)\n",
        "\n",
        "        #msgShow = ''\n",
        "        #for i in range(len(commitMsg)):\n",
        "        #    msgShow += commitMsg[i]\n",
        "        #print(msgShow)\n",
        "\n",
        "        return commitMsg\n",
        "\n",
        "    def ReadDiffLines(filename):\n",
        "        '''\n",
        "        Read diff code from a file.\n",
        "        :param filename:  file name (string).\n",
        "        :return: diffLines - diff code.\n",
        "        [['line', ...], ['line', ...], ...]\n",
        "        '''\n",
        "\n",
        "        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n",
        "        lines = fp.readlines()  # read all lines.\n",
        "        numLines = len(lines)  # get the line number.\n",
        "        # print(lines)\n",
        "\n",
        "        atLines = [i for i in range(numLines) if lines[i].startswith('@@ ')]  # find all lines start with @@.\n",
        "        atLines.append(numLines)\n",
        "        # print(atLines)\n",
        "\n",
        "        diffLines = []\n",
        "        for nh in range(len(atLines) - 1):  # find all hunks.\n",
        "            # print(atLines[nh], atLines[nh + 1])\n",
        "            hunk = []\n",
        "            for nl in range(atLines[nh] + 1, atLines[nh + 1]):\n",
        "                # print(lines[nl], end='')\n",
        "                if lines[nl].startswith('diff --git '):\n",
        "                    break\n",
        "                else:\n",
        "                    hunk.append(lines[nl])\n",
        "            diffLines.append(hunk)\n",
        "            # print(hunk)\n",
        "        # print(diffLines)\n",
        "        # print(len(diffLines))\n",
        "\n",
        "        # process the last hunk.\n",
        "        lastHunk = diffLines[-1]\n",
        "        numLastHunk = len(lastHunk)\n",
        "        dashLines = [i for i in range(numLastHunk) if lastHunk[i].startswith('--')]\n",
        "        if (len(dashLines)):\n",
        "            lnum = dashLines[-1]\n",
        "            for i in reversed(range(lnum, numLastHunk)):\n",
        "                lastHunk.pop(i)\n",
        "        # print(diffLines)\n",
        "        # print(len(diffLines))\n",
        "\n",
        "        return diffLines\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'filelist.txt', 'w')\n",
        "\n",
        "    # initialize data.\n",
        "    data = []\n",
        "    # read security patch data.\n",
        "    for root, ds, fs in os.walk(sDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 1])\n",
        "\n",
        "    # read positive data.\n",
        "    for root, ds, fs in os.walk(pDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 1])\n",
        "\n",
        "    # read negative data.\n",
        "    for root, ds, fs in os.walk(nDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 0])\n",
        "    fp.close()\n",
        "\n",
        "    #print(len(dataLoaded))\n",
        "    #print(len(dataLoaded[0]))\n",
        "    #print(dataLoaded)\n",
        "    # [[['a', 'b', 'c', ], [['', '', '', ], ['', '', '', ], ], 0/1], ]\n",
        "    # sample = dataLoaded[i]\n",
        "    # commitMsg = dataLoaded[i][0]\n",
        "    # diffLines = dataLoaded[i][1]\n",
        "    # label = dataLoaded[i][2]\n",
        "    # diffHunk = dataLoaded[i][1][j]\n",
        "\n",
        "    # save dataLoaded.\n",
        "    if not os.path.exists(tempPath + '/data.npy'):\n",
        "        np.save(tempPath + '/data.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Save ' + str(len(data)) + ' raw data to ' + tempPath + '/data.npy.')\n",
        "\n",
        "    return data\n",
        "\n",
        "def GetDiffProps(data):\n",
        "    '''\n",
        "    Get the properties of the code in diff files.\n",
        "    :param data: [[[line, , ], [[line, , ], [line, , ], ...], 0/1], ...]\n",
        "    :return: props - [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def RemoveSign(line):\n",
        "        '''\n",
        "        Remove the sign (+/-) in the first character.\n",
        "        :param line: a code line.\n",
        "        :return: process line.\n",
        "        '''\n",
        "\n",
        "        return ' ' + line[1:] if (line[0] == '+') or (line[0] == '-') else line\n",
        "\n",
        "    def GetClangTokens(line):\n",
        "        '''\n",
        "        Get the tokens of a line with the Clang tool.\n",
        "        :param line: a code line.\n",
        "        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n",
        "                 tokenTypes - [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n",
        "                 diffTypes - [dft, dft, ...] (dft: -1, 0, 1)\n",
        "        '''\n",
        "\n",
        "        # remove non-ascii\n",
        "        line = line.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "        # defination.\n",
        "        tokenClass = [clang.cindex.TokenKind.KEYWORD,      # 1\n",
        "                      clang.cindex.TokenKind.IDENTIFIER,   # 2\n",
        "                      clang.cindex.TokenKind.LITERAL,      # 3\n",
        "                      clang.cindex.TokenKind.PUNCTUATION,  # 4\n",
        "                      clang.cindex.TokenKind.COMMENT]      # 5\n",
        "        tokenDict = {cls: index + 1 for index, cls in enumerate(tokenClass)}\n",
        "        #print(tokenDict)\n",
        "\n",
        "        # initialize.\n",
        "        tokens = []\n",
        "        tokenTypes = []\n",
        "        diffTypes = []\n",
        "\n",
        "        # clang sparser.\n",
        "        idx = clang.cindex.Index.create()\n",
        "        tu = idx.parse('tmp.cpp', args=['-std=c++11'], unsaved_files=[('tmp.cpp', RemoveSign(line))], options=0)\n",
        "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
        "            #print(t.kind, t.spelling, t.location)\n",
        "            tokens.append(t.spelling)\n",
        "            tokenTypes.append(tokenDict[t.kind])\n",
        "            diffTypes.append(1 if (line[0] == '+') else -1 if (line[0] == '-') else 0)\n",
        "        #print(tokens)\n",
        "        #print(tokenTypes)\n",
        "        #print(diffTypes)\n",
        "\n",
        "        return tokens, tokenTypes, diffTypes\n",
        "\n",
        "    def GetWordTokens(line):\n",
        "        '''\n",
        "        Get the word tokens from a code line.\n",
        "        :param line: a code line.\n",
        "        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n",
        "        '''\n",
        "\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(RemoveSign(line))\n",
        "        return tokens\n",
        "\n",
        "    def GetString(lines):\n",
        "        '''\n",
        "        Get the strings from the diff code\n",
        "        :param lines: diff code.\n",
        "        :return: lineStr - All the diff lines.\n",
        "                 lineStrB - The before-version code lines.\n",
        "                 lineStrA - The after-version code lines.\n",
        "        '''\n",
        "\n",
        "        lineStr = ''\n",
        "        lineStrB = ''\n",
        "        lineStrA = ''\n",
        "        for hunk in lines:\n",
        "            for line in hunk:\n",
        "                # all lines.\n",
        "                lineStr += RemoveSign(line)\n",
        "                # all Before lines.\n",
        "                lineStrB += RemoveSign(line) if line[0] != '+' else ''\n",
        "                # all After lines.\n",
        "                lineStrA += RemoveSign(line) if line[0] != '-' else ''\n",
        "\n",
        "        return lineStr, lineStrB, lineStrA\n",
        "\n",
        "    def GetDiffTokens(lines):\n",
        "        '''\n",
        "        Get the tokens for the diff lines.\n",
        "        :param lines: the diff code.\n",
        "        :return: tokens - tokens ['tk', 'tk', ...] ('tk': string)\n",
        "                 tokenTypes - token types [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n",
        "                 diffTypes - diff types [dft, dft, ...] (dft: -1, 0, 1)\n",
        "        '''\n",
        "\n",
        "        # initialize.\n",
        "        tokens = []\n",
        "        tokenTypes = []\n",
        "        diffTypes = []\n",
        "\n",
        "        # for each line of lines.\n",
        "        for hunk in lines:\n",
        "            for line in hunk:\n",
        "                #print(line, end='')\n",
        "                tk, tkT, dfT = GetClangTokens(line)\n",
        "                tokens.extend(tk)\n",
        "                tokenTypes.extend(tkT)\n",
        "                diffTypes.extend(dfT)\n",
        "                #print('-----------------------------------------------------------------------')\n",
        "        #print(tokens)\n",
        "        #print(tokenTypes)\n",
        "        #print(diffTypes)\n",
        "\n",
        "        return tokens, tokenTypes, diffTypes\n",
        "\n",
        "    #lines = data[0][1]\n",
        "    #print(lines)\n",
        "    #hunk = data[0][1][0]\n",
        "    #print(hunk)\n",
        "    #line = data[0][1][0][0]\n",
        "    #print(line)\n",
        "\n",
        "    # for each sample data[n].\n",
        "    numData = len(data)\n",
        "    props = []\n",
        "    for n in range(numData):\n",
        "        # get the lines of the diff file.\n",
        "        diffLines = data[n][1]\n",
        "        # properties.\n",
        "        tk, tkT, dfT = GetDiffTokens(diffLines)\n",
        "        label = data[n][2]\n",
        "        prop = [tk, tkT, dfT, label]\n",
        "        #print(prop)\n",
        "        props.append(prop)\n",
        "        print(n)\n",
        "\n",
        "    # save props.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    if not os.path.exists(tempPath + '/props.npy'):\n",
        "        np.save(tempPath + '/props.npy', props, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Save ' + str(len(props)) + ' diff property data to ' + tempPath + '/props.npy.')\n",
        "\n",
        "    return props\n",
        "\n",
        "def GetDiffVocab(props):\n",
        "    '''\n",
        "    Get the vocabulary of diff tokens\n",
        "    :param props - the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :return: vocab - the vocabulary of diff tokens. ['tk', 'tk', ...]\n",
        "             maxLen - the max length of a diff code.\n",
        "    '''\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'difflen.csv', 'w')\n",
        "\n",
        "    # get the whole tokens and the max diff length.\n",
        "    tokens = []\n",
        "    maxLen = 0\n",
        "\n",
        "    # for each sample.\n",
        "    for item in props:\n",
        "        tokens.extend(item[0])\n",
        "        maxLen = len(item[0]) if (len(item[0]) > maxLen) else maxLen\n",
        "        fp.write(str(len(item[0])) + '\\n')\n",
        "    fp.close()\n",
        "\n",
        "    # remove duplicates and get vocabulary.\n",
        "    vocab = {}.fromkeys(tokens)\n",
        "    vocab = list(vocab.keys())\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffVocab> There are ' + str(len(vocab)) + ' diff vocabulary tokens. (except \\'<pad>\\')')\n",
        "    print('[INFO] <GetDiffVocab> The max diff length is ' + str(maxLen) + ' tokens. (hyperparameter: _DiffMaxLen_ = ' + str(_DiffMaxLen_) + ')')\n",
        "\n",
        "    return vocab, maxLen\n",
        "\n",
        "def GetDiffDict(vocab):\n",
        "    '''\n",
        "    Get the dictionary of diff vocabulary.\n",
        "    :param vocab: the vocabulary of diff tokens. ['tk', 'tk', ...]\n",
        "    :return: tokenDict - the dictionary of diff vocabulary.\n",
        "    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n",
        "    '''\n",
        "\n",
        "    # get token dict from vocabulary.\n",
        "    tokenDict = {token: (index+1) for index, token in enumerate(vocab)}\n",
        "    tokenDict['<pad>'] = 0\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffDict> Create dictionary for ' + str(len(tokenDict)) + ' diff vocabulary tokens. (with \\'<pad>\\')')\n",
        "\n",
        "    return tokenDict\n",
        "\n",
        "def GetDiffEmbed(tokenDict, embedSize):\n",
        "    '''\n",
        "    Get the pre-trained weights for embedding layer from the dictionary of diff vocabulary.\n",
        "    :param tokenDict: the dictionary of diff vocabulary.\n",
        "    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n",
        "    :param embedSize: the dimension of the embedding vector.\n",
        "    :return: preWeights - the pre-trained weights for embedding layer.\n",
        "    [[n, ...], [n, ...], ...]\n",
        "    '''\n",
        "\n",
        "    # number of the vocabulary tokens.\n",
        "    numTokens = len(tokenDict)\n",
        "\n",
        "    # initialize the pre-trained weights for embedding layer.\n",
        "    preWeights = np.zeros((numTokens, embedSize))\n",
        "    for index in range(numTokens):\n",
        "        preWeights[index] = np.random.normal(size=(embedSize,))\n",
        "    print('[INFO] <GetDiffEmbed> Create pre-trained embedding weights with ' + str(len(preWeights)) + ' * ' + str(len(preWeights[0])) + ' matrix.')\n",
        "\n",
        "    # save preWeights.\n",
        "    if not os.path.exists(tempPath + '/preWeights.npy'):\n",
        "        np.save(tempPath + '/preWeights.npy', preWeights, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffEmbed> Save the pre-trained weights of embedding layer to ' + tempPath + '/preWeights.npy.')\n",
        "\n",
        "    return preWeights\n",
        "\n",
        "def GetDiffMapping(props, maxLen, tokenDict):\n",
        "    '''\n",
        "    Map the feature data into indexed data.\n",
        "    :param props: the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :param maxLen: the max length of a diff code.\n",
        "    :param tokenDict: the dictionary of diff vocabulary.\n",
        "    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n",
        "    :return: np.array(data) - feature data.\n",
        "             [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "             np.array(labels) - labels.\n",
        "             [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def PadList(dList, pad, length):\n",
        "        '''\n",
        "        Pad the list data to a fixed length.\n",
        "        :param dList: the list data - [ , , ...]\n",
        "        :param pad: the variable used to pad.\n",
        "        :param length: the fixed length.\n",
        "        :return: dList - padded list data. [ , , ...]\n",
        "        '''\n",
        "\n",
        "        if len(dList) <= length:\n",
        "            dList.extend(pad for i in range(length - len(dList)))\n",
        "        elif len(dList) > length:\n",
        "            dList = dList[0:length]\n",
        "\n",
        "        return dList\n",
        "\n",
        "    # initialize the data and labels.\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # for each sample.\n",
        "    for item in props:\n",
        "        # initialize sample.\n",
        "        sample = []\n",
        "\n",
        "        # process token.\n",
        "        tokens = item[0]\n",
        "        tokens = PadList(tokens, '<pad>', maxLen)\n",
        "        tokens2index = []\n",
        "        for tk in tokens:\n",
        "            tokens2index.append(tokenDict[tk])\n",
        "        sample.append(tokens2index)\n",
        "        # process tokenTypes.\n",
        "        tokenTypes = item[1]\n",
        "        tokenTypes = PadList(tokenTypes, 0, maxLen)\n",
        "        sample.append(tokenTypes)\n",
        "        # process diffTypes.\n",
        "        diffTypes = item[2]\n",
        "        diffTypes = PadList(diffTypes, 0, maxLen)\n",
        "        sample.append(diffTypes)\n",
        "\n",
        "        # process sample.\n",
        "        sample = np.array(sample).T\n",
        "        data.append(sample)\n",
        "        # process label.\n",
        "        label = item[3]\n",
        "        labels.append([label])\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] data:')\n",
        "        print(data[0:3])\n",
        "        print('[DEBUG] labels:')\n",
        "        print(labels[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffMapping> Create ' + str(len(data)) + ' feature data with ' + str(len(data[0])) + ' * ' + str(len(data[0][0])) + ' matrix.')\n",
        "    print('[INFO] <GetDiffMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/ndata_' + str(maxLen) + '.npy')) \\\n",
        "            | (not os.path.exists(tempPath + '/nlabels_' + str(maxLen) + '.npy')):\n",
        "        np.save(tempPath + '/ndata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffMapping> Save the mapped numpy data to ' + tempPath + '/ndata_' + str(maxLen) + '.npy.')\n",
        "        np.save(tempPath + '/nlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffMapping> Save the mapped numpy labels to ' + tempPath + '/nlabels_' + str(maxLen) + '.npy.')\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def UpdateTokenTypes(data):\n",
        "    '''\n",
        "    Update the token type in the feature data into one-hot vector.\n",
        "    :param data: feature data. [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "    :return: np.array(newData). [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    '''\n",
        "\n",
        "    newData = []\n",
        "    # for each sample.\n",
        "    for item in data:\n",
        "        # get the transpose of props.\n",
        "        itemT = item.T\n",
        "        # initialize new sample.\n",
        "        newItem = []\n",
        "        newItem.append(itemT[0])\n",
        "        newItem.extend(np.zeros((5, len(item)), dtype=int))\n",
        "        newItem.append(itemT[2])\n",
        "        # assign the new sample.\n",
        "        for i in range(len(item)):\n",
        "            tokenType = itemT[1][i]\n",
        "            if (tokenType):\n",
        "                newItem[tokenType][i] = 1\n",
        "        # get the transpose of new sample.\n",
        "        newItem = np.array(newItem).T\n",
        "        # append new sample.\n",
        "        newData.append(newItem)\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] newData:')\n",
        "        print(newData[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <UpdateTokenTypes> Update ' + str(len(newData)) + ' feature data with ' + str(len(newData[0])) + ' * ' + str(len(newData[0][0])) + ' matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/newdata_' + str(len(newData[0])) + '.npy')):\n",
        "        np.save(tempPath + '/newdata_' + str(len(newData[0])) + '.npy', newData, allow_pickle=True)\n",
        "        print('[INFO] <UpdateTokenTypes> Save the mapped numpy data to ' + tempPath + '/newdata_' + str(len(newData[0])) + '.npy.')\n",
        "\n",
        "    # change marco.\n",
        "    global _DiffExtraDim_\n",
        "    _DiffExtraDim_ = 6\n",
        "\n",
        "    return np.array(newData)\n",
        "\n",
        "def SplitData(data, labels, setType, rate=0.2):\n",
        "    '''\n",
        "    Split the data and labels into two sets with a specific rate.\n",
        "    :param data: feature data.\n",
        "    [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "    [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param labels: labels. [[0/1], ...]\n",
        "    :param setType: the splited dataset type.\n",
        "    :param rate: the split rate. 0 ~ 1\n",
        "    :return: dsetRest - the rest dataset.\n",
        "             lsetRest - the rest labels.\n",
        "             dset - the splited dataset.\n",
        "             lset - the splited labels.\n",
        "    '''\n",
        "\n",
        "    # set parameters.\n",
        "    setType = setType.upper()\n",
        "    numData = len(data)\n",
        "    num = math.floor(numData * rate)\n",
        "\n",
        "    # get the random data list.\n",
        "    if (os.path.exists(tempPath + '/split_' + setType + '.npy')) & (_LOCK_):\n",
        "        dataList = np.load(tempPath + '/split_' + setType + '.npy')\n",
        "    else:\n",
        "        dataList = list(range(numData))\n",
        "        random.shuffle(dataList)\n",
        "        np.save(tempPath + '/split_' + setType + '.npy', dataList, allow_pickle=True)\n",
        "\n",
        "    # split data.\n",
        "    dset = data[dataList[0:num]]\n",
        "    lset = labels[dataList[0:num]]\n",
        "    dsetRest = data[dataList[num:]]\n",
        "    lsetRest = labels[dataList[num:]]\n",
        "\n",
        "    # print.\n",
        "    setTypeRest = 'TRAIN' if (setType == 'VALID') else 'REST'\n",
        "    print('[INFO] <SplitData> Split data into ' + str(len(dsetRest)) + ' ' + setTypeRest\n",
        "          + ' dataset and ' + str(len(dset)) + ' ' + setType + ' dataset. (Total: '\n",
        "          + str(len(dsetRest) + len(dset)) + ', Rate: ' + str(int(rate * 100)) + '%)')\n",
        "\n",
        "    return dsetRest, lsetRest, dset, lset\n",
        "\n",
        "class DiffRNN(nn.Module):\n",
        "    '''\n",
        "    DiffRNN : convert a text data into a predicted label.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, preWeights, hiddenSize=32, hiddenLayers=1):\n",
        "        '''\n",
        "        define each layer in the network model.\n",
        "        :param preWeights: tensor pre-trained weights for embedding layer.\n",
        "        :param hiddenSize: node number in the hidden layer.\n",
        "        :param hiddenLayers: number of hidden layer.\n",
        "        '''\n",
        "\n",
        "        super(DiffRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "        vocabSize, embedDim = preWeights.size()\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabSize, embedding_dim=embedDim)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        # LSTM Layer\n",
        "        #_DiffExtraDim_ = 6\n",
        "        if _DEBUG_: print(_DiffExtraDim_)\n",
        "        self.lstm = nn.LSTM(input_size=embedDim+_DiffExtraDim_, hidden_size=hiddenSize, num_layers=hiddenLayers, bidirectional=True)\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(hiddenSize * hiddenLayers * 2, class_num)\n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convert inputs to predictions.\n",
        "        :param x: input tensor. dimension: batch_size * diff_length * feature_dim.\n",
        "        :return: self.softmax(final_out) - predictions.\n",
        "        [[0.3, 0.7], [0.2, 0.8], ...]\n",
        "        '''\n",
        "\n",
        "        # x             batch_size * diff_length * feature_dim\n",
        "        embeds = self.embedding(x[:,:,0])\n",
        "        # embeds        batch_size * diff_length * embedding_dim\n",
        "        features = x[:, :, 1:]\n",
        "        # features      batch_size * diff_length * _DiffExtraDim_\n",
        "        inputs = torch.cat((embeds.float(), features.float()), 2)\n",
        "        # inputs        batch_size * diff_length * (embedding_dim + _DiffExtraDim_)\n",
        "        inputs = inputs.permute(1, 0, 2)\n",
        "        # inputs        diff_length * batch_size * (embedding_dim + _DiffExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(inputs)\n",
        "        # lstm_out      diff_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        feature_map = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # feature_map   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        final_out = self.fc(feature_map)    # batch_size * class_num\n",
        "        return self.softmax(final_out)      # batch_size * class_num\n",
        "\n",
        "def DiffRNNTrain(dTrain, lTrain, dValid, lValid, preWeights, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
        "    '''\n",
        "    Train the DiffRNN model.\n",
        "    :param dTrain: training data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param lTrain: training label. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param dValid: validation data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param lValid: validation label. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param preWeights: pre-trained weights for embedding layer.\n",
        "    :param batchsize: number of samples in a batch.\n",
        "    :param learnRate: learning rate.\n",
        "    :param dTest: test data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param lTest: test label. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :return: model - the DiffRNN model.\n",
        "    '''\n",
        "\n",
        "    # get the mark of the test dataset.\n",
        "    if dTest is None: dTest = []\n",
        "    if lTest is None: lTest = []\n",
        "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    if (markTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "    if (markTest):\n",
        "        test = torchdata.TensorDataset(xTest, yTest)\n",
        "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights.\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(2):\n",
        "        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = DiffRNN(preWeights, hiddenSize=_DRnnHidSiz_, hiddenLayers=_DRnnHidLay_)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[INFO] <DiffRNNTrain> ModelType: DiffRNN, HiddenNodes: %d, HiddenLayers: %d.' % (_DRnnHidSiz_, _DRnnHidLay_))\n",
        "    print('[INFO] <DiffRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d.' % (batchsize, learnRate, _DRnnMaxEpoch_, _DRnnPerEpoch_))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(_DRnnMaxEpoch_):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # back propagation.\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                # data conversion.\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                # forward propagation.\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # testing phase.\n",
        "        if (markTest):\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with torch.no_grad():\n",
        "                for iter, (data, label) in enumerate(testloader):\n",
        "                    # data conversion.\n",
        "                    data = data.to(device)\n",
        "                    label = label.contiguous().view(-1)\n",
        "                    label = label.to(device)\n",
        "                    # forward propagation.\n",
        "                    yhat = model.forward(data)  # get output\n",
        "                    # statistic\n",
        "                    preds = yhat.max(1)[1]\n",
        "                    predictions.extend(preds.int().tolist())\n",
        "                    labels.extend(label.int().tolist())\n",
        "                    torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # test accuracy.\n",
        "            accTest = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # output information.\n",
        "        if (0 == (epoch + 1) % _DRnnPerEpoch_):\n",
        "            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
        "            if (markTest):\n",
        "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
        "            print(strAcc)\n",
        "        # save the best model.\n",
        "        if (accList[-1] > max(accList[0:-1])):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_DiffRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch >= _DRnnJudEpoch_) and (accList[-1] < min(accList[-1-_DRnnJudEpoch_:-1])):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_DiffRNN.pth'))\n",
        "    print('[INFO] <DiffRNNTrain> Finish training DiffRNN model. (Best model: ' + tempPath + '/model_DiffRNN.pth)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def DiffRNNTest(model, dTest, lTest, batchsize=64):\n",
        "    '''\n",
        "    Test the DiffRNN model.\n",
        "    :param model: deep learning model.\n",
        "    :param dTest: test data.\n",
        "    :param lTest: test label.\n",
        "    :param batchsize: number of samples in a batch\n",
        "    :return: predictions - predicted labels. [[0], [1], ...]\n",
        "             accuracy - the total test accuracy. numeric\n",
        "    '''\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # load the model of recurrent neural network.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # forward propagation.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "\n",
        "    return predictions, accuracy\n",
        "\n",
        "def OutputEval(predictions, labels, method=''):\n",
        "    '''\n",
        "    Output the evaluation results.\n",
        "    :param predictions: predicted labels. [[0], [1], ...]\n",
        "    :param labels: ground truth labels. [[1], [1], ...]\n",
        "    :param method: method name. string\n",
        "    :return: accuracy - the total accuracy. numeric\n",
        "             confusion - confusion matrix [[1000, 23], [12, 500]]\n",
        "    '''\n",
        "\n",
        "    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n",
        "    def Evaluation(predictions, labels):\n",
        "\n",
        "        # parameter settings.\n",
        "        D = len(labels)\n",
        "        cls = 2\n",
        "\n",
        "        # get confusion matrix.\n",
        "        confusion = np.zeros((cls, cls))\n",
        "        for ind in range(D):\n",
        "            nRow = int(predictions[ind][0])\n",
        "            nCol = int(labels[ind][0])\n",
        "            confusion[nRow][nCol] += 1\n",
        "\n",
        "        # get accuracy.\n",
        "        accuracy = 0\n",
        "        for ind in range(cls):\n",
        "            accuracy += confusion[ind][ind]\n",
        "        accuracy /= D\n",
        "\n",
        "        return accuracy, confusion\n",
        "\n",
        "    # get accuracy and confusion matrix.\n",
        "    accuracy, confusion = Evaluation(predictions, labels)\n",
        "    precision = confusion[1][1] / (confusion[1][0] + confusion[1][1]) if (confusion[1][0] + confusion[1][1]) else 0\n",
        "    recall = confusion[1][1] / (confusion[0][1] + confusion[1][1]) if (confusion[0][1] + confusion[1][1]) else 0\n",
        "    F1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "    # output on screen and to file.\n",
        "    print('       -------------------------------------------')\n",
        "    print('       method           :  ' +  method) if len(method) else print('', end='')\n",
        "    print('       accuracy  (ACC)  :  %.3f%%' % (accuracy * 100))\n",
        "    print('       precision (P)    :  %.3f%%' % (precision * 100))\n",
        "    print('       recall    (R)    :  %.3f%%' % (recall * 100))\n",
        "    print('       F1 score  (F1)   :  %.3f' % (F1))\n",
        "    print('       fall-out  (FPR)  :  %.3f%%' % (confusion[1][0] * 100 / (confusion[1][0] + confusion[0][0])))\n",
        "    print('       miss rate (FNR)  :  %.3f%%' % (confusion[0][1] * 100 / (confusion[0][1] + confusion[1][1])))\n",
        "    print('       confusion matrix :      (actual)')\n",
        "    print('                           Neg         Pos')\n",
        "    print('       (predicted) Neg     %-5d(TN)   %-5d(FN)' % (confusion[0][0], confusion[0][1]))\n",
        "    print('                   Pos     %-5d(FP)   %-5d(TP)' % (confusion[1][0], confusion[1][1]))\n",
        "    print('       -------------------------------------------')\n",
        "\n",
        "    return accuracy, confusion\n",
        "\n",
        "def demoCommitMsg():\n",
        "    '''\n",
        "    demo program of using commit message to identify patches.\n",
        "    '''\n",
        "\n",
        "    # load data.\n",
        "    if (not os.path.exists(tempPath + '/data.npy')):  # | (not _DEBUG_)\n",
        "        dataLoaded = ReadData()\n",
        "    else:\n",
        "        dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n",
        "\n",
        "    # get the commit messages from data.\n",
        "    if (not os.path.exists(tempPath + '/msgs.npy')):\n",
        "        commitMsgs = GetCommitMsgs(dataLoaded)\n",
        "    else:\n",
        "        commitMsgs = np.load(tempPath + '/msgs.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetCommitMsg> Load ' + str(len(commitMsgs)) + ' commit messages from ' + tempPath + '/msgs.npy.')\n",
        "\n",
        "    # get the message token vocabulary.\n",
        "    msgVocab, msgMaxLen = GetMsgVocab(commitMsgs)\n",
        "    # get the max msg length.\n",
        "    msgMaxLen = _MsgMaxLen_ if (msgMaxLen > _MsgMaxLen_) else msgMaxLen\n",
        "    # get the msg token dictionary.\n",
        "    msgDict = GetMsgDict(msgVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    msgPreWeights = GetMsgEmbed(msgDict, _MsgEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    msgData, msgLabels = GetMsgMapping(commitMsgs, msgMaxLen, msgDict)\n",
        "    # split data into rest/test dataset.\n",
        "    mdataTrain, mlabelTrain, mdataTest, mlabelTest = SplitData(msgData, msgLabels, 'test', rate=0.2)\n",
        "\n",
        "    # MsgRNNTrain\n",
        "    if (_MODEL_) & (os.path.exists(tempPath + '/model_MsgRNN.pth')):\n",
        "        preWeights = torch.from_numpy(msgPreWeights)\n",
        "        model = MsgRNN(preWeights, hiddenSize=_MRnnHidSiz_, hiddenLayers=_MRnnHidLay_)\n",
        "        model.load_state_dict(torch.load(tempPath + '/model_MsgRNN.pth'))\n",
        "    else:\n",
        "        model = MsgRNNTrain(mdataTrain, mlabelTrain, mdataTest, mlabelTest, msgPreWeights,\n",
        "                            batchsize=_MRnnBatchSz_, learnRate=_MRnnLearnRt_, dTest=mdataTest, lTest=mlabelTest)\n",
        "\n",
        "    # MsgRNNTest\n",
        "    predictions, accuracy = MsgRNNTest(model, mdataTest, mlabelTest, batchsize=_MRnnBatchSz_)\n",
        "    _, confusion = OutputEval(predictions, mlabelTest, 'MsgRNN')\n",
        "\n",
        "    return\n",
        "\n",
        "def GetCommitMsgs(data):\n",
        "    '''\n",
        "    Get the commit messages in diff files.\n",
        "    :param data: [[[line, , ], [[line, , ], [line, , ], ...], 0/1], ...]\n",
        "    :return: msgs - [[[tokens], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def GetMsgTokens(lines):\n",
        "        '''\n",
        "        Get the tokens from a commit message.\n",
        "        :param lines: commit message. [line, , ]\n",
        "        :return: tokensStem ['tk', , ]\n",
        "        '''\n",
        "\n",
        "        # concatenate lines.\n",
        "        # get the string of commit message.\n",
        "        msg = ''\n",
        "        for line in lines:\n",
        "            msg += line[:-1] + ' '\n",
        "        #print(msg)\n",
        "\n",
        "        # pre-process.\n",
        "        # remove url.\n",
        "        pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        msg = re.sub(pattern, ' ', msg)\n",
        "        # remove independent numbers.\n",
        "        pattern = r' \\d+ '\n",
        "        msg = re.sub(pattern, ' ', msg)\n",
        "        # lower case capitalized words.\n",
        "        pattern = r'([A-Z][a-z]+)'\n",
        "        def LowerFunc(matched):\n",
        "            return matched.group(1).lower()\n",
        "        msg = re.sub(pattern, LowerFunc, msg)\n",
        "        # remove footnote.\n",
        "        patterns = ['signed-off-by:', 'reported-by:', 'reviewed-by:', 'acked-by:', 'found-by:', 'tested-by:', 'cc:']\n",
        "        for pattern in patterns:\n",
        "            index = msg.find(pattern)\n",
        "            if (index > 0):\n",
        "                msg = msg[:index]\n",
        "        #print(msg)\n",
        "\n",
        "        # clearance.\n",
        "        # get the tokens.\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(msg)\n",
        "        # clear tokens that don't contain any english letter.\n",
        "        for i in reversed(range(len(tokens))):\n",
        "            if not (re.search('[a-z]', tokens[i])):\n",
        "                tokens.pop(i)\n",
        "        # clear tokens that are stopwords.\n",
        "        for i in reversed(range(len(tokens))):\n",
        "            if (tokens[i] in stopwords.words('english')):\n",
        "                tokens.pop(i)\n",
        "        pattern = re.compile(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\")\n",
        "        for i in reversed(range(len(tokens))):\n",
        "            if (pattern.findall(tokens[i])):\n",
        "                tokens.pop(i)\n",
        "        #print(tokens)\n",
        "\n",
        "        # process tokens with stemming.\n",
        "        porter = PorterStemmer()\n",
        "        tokensStem = []\n",
        "        for item in tokens:\n",
        "            tokensStem.append(porter.stem(item))\n",
        "        #print(tokensStem)\n",
        "\n",
        "        return tokensStem\n",
        "\n",
        "    # for each sample data[n].\n",
        "    numData = len(data)\n",
        "    msgs = []\n",
        "    for n in range(numData):\n",
        "        # get the lines of the commit message.\n",
        "        commitMsg = data[n][0]\n",
        "        mtk = GetMsgTokens(commitMsg)\n",
        "        # get the label.\n",
        "        label = data[n][2]\n",
        "        #print([mtk, label])\n",
        "        # append the message tokens.\n",
        "        msgs.append([mtk, label])\n",
        "        print(n)\n",
        "\n",
        "    # save commit messages.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    if not os.path.exists(tempPath + '/msgs.npy'):\n",
        "        np.save(tempPath + '/msgs.npy', msgs, allow_pickle=True)\n",
        "        print('[INFO] <GetCommitMsg> Save ' + str(len(msgs)) + ' commit messages to ' + tempPath + '/msgs.npy.')\n",
        "\n",
        "    return msgs\n",
        "\n",
        "def GetMsgVocab(msgs):\n",
        "    '''\n",
        "    Get the vocabulary of message tokens\n",
        "    :param msgs - [[[tokens], 0/1], ...]\n",
        "    :return: vocab - the vocabulary of message tokens. ['tk', 'tk', ...]\n",
        "             maxLen - the max length of a commit message.\n",
        "    '''\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'msglen.csv', 'w')\n",
        "\n",
        "    # get the whole tokens and the max msg length.\n",
        "    tokens = []\n",
        "    maxLen = 0\n",
        "\n",
        "    # for each sample.\n",
        "    for item in msgs:\n",
        "        tokens.extend(item[0])\n",
        "        maxLen = len(item[0]) if (len(item[0]) > maxLen) else maxLen\n",
        "        fp.write(str(len(item[0])) + '\\n')\n",
        "    fp.close()\n",
        "\n",
        "    # remove duplicates and get vocabulary.\n",
        "    vocab = {}.fromkeys(tokens)\n",
        "    vocab = list(vocab.keys())\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetMsgVocab> There are ' + str(len(vocab)) + ' commit message vocabulary tokens. (except \\'<pad>\\')')\n",
        "    print('[INFO] <GetMsgVocab> The max msg length is ' + str(maxLen) + ' tokens. (hyperparameter: _MsgMaxLen_ = ' + str(_MsgMaxLen_) + ')')\n",
        "\n",
        "    return vocab, maxLen\n",
        "\n",
        "def GetMsgDict(vocab):\n",
        "    '''\n",
        "    Get the dictionary of msg vocabulary.\n",
        "    :param vocab: the vocabulary of msg tokens. ['tk', 'tk', ...]\n",
        "    :return: tokenDict - the dictionary of msg vocabulary.\n",
        "    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n",
        "    '''\n",
        "\n",
        "    # get token dict from vocabulary.\n",
        "    tokenDict = {token: (index+1) for index, token in enumerate(vocab)}\n",
        "    tokenDict['<pad>'] = 0\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetMsgDict> Create dictionary for ' + str(len(tokenDict)) + ' msg vocabulary tokens. (with \\'<pad>\\')')\n",
        "\n",
        "    return tokenDict\n",
        "\n",
        "def GetMsgEmbed(tokenDict, embedSize):\n",
        "    '''\n",
        "    Get the pre-trained weights for embedding layer from the dictionary of msg vocabulary.\n",
        "    :param tokenDict: the dictionary of msg vocabulary.\n",
        "    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n",
        "    :param embedSize: the dimension of the embedding vector.\n",
        "    :return: preWeights - the pre-trained weights for embedding layer.\n",
        "    [[n, ...], [n, ...], ...]\n",
        "    '''\n",
        "\n",
        "    # number of the vocabulary tokens.\n",
        "    numTokens = len(tokenDict)\n",
        "\n",
        "    # initialize the pre-trained weights for embedding layer.\n",
        "    preWeights = np.zeros((numTokens, embedSize))\n",
        "    for index in range(numTokens):\n",
        "        preWeights[index] = np.random.normal(size=(embedSize,))\n",
        "    print('[INFO] <GetMsgEmbed> Create pre-trained embedding weights with ' + str(len(preWeights)) + ' * ' + str(len(preWeights[0])) + ' matrix.')\n",
        "\n",
        "    # save preWeights.\n",
        "    if not os.path.exists(tempPath + '/msgPreWeights.npy'):\n",
        "        np.save(tempPath + '/msgPreWeights.npy', preWeights, allow_pickle=True)\n",
        "        print('[INFO] <GetMsgEmbed> Save the pre-trained weights of embedding layer to ' + tempPath + '/msgPreWeights.npy.')\n",
        "\n",
        "    return preWeights\n",
        "\n",
        "def GetMsgMapping(msgs, maxLen, tokenDict):\n",
        "    '''\n",
        "    Map the feature data into indexed data.\n",
        "    :param props: the features of commit messages.\n",
        "    [[[tokens], 0/1], ...]\n",
        "    :param maxLen: the max length of the commit message.\n",
        "    :param tokenDict: the dictionary of commit message vocabulary.\n",
        "    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n",
        "    :return: np.array(data) - feature data.\n",
        "             [[n, ...], ...]\n",
        "             np.array(labels) - labels.\n",
        "             [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def PadList(dList, pad, length):\n",
        "        '''\n",
        "        Pad the list data to a fixed length.\n",
        "        :param dList: the list data - [ , , ...]\n",
        "        :param pad: the variable used to pad.\n",
        "        :param length: the fixed length.\n",
        "        :return: dList - padded list data. [ , , ...]\n",
        "        '''\n",
        "\n",
        "        if len(dList) <= length:\n",
        "            dList.extend(pad for i in range(length - len(dList)))\n",
        "        elif len(dList) > length:\n",
        "            dList = dList[0:length]\n",
        "\n",
        "        return dList\n",
        "\n",
        "    # initialize the data and labels.\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # for each sample.\n",
        "    for item in msgs:\n",
        "        # process tokens.\n",
        "        tokens = item[0]\n",
        "        tokens = PadList(tokens, '<pad>', maxLen)\n",
        "        # convert tokens into numbers.\n",
        "        tokens2index = []\n",
        "        for tk in tokens:\n",
        "            tokens2index.append(tokenDict[tk])\n",
        "        data.append(tokens2index)\n",
        "        # process label.\n",
        "        label = item[1]\n",
        "        labels.append([label])\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] data:')\n",
        "        print(data[0:3])\n",
        "        print('[DEBUG] labels:')\n",
        "        print(labels[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetMsgMapping> Create ' + str(len(data)) + ' feature data with 1 * ' + str(len(data[0])) + ' vector.')\n",
        "    print('[INFO] <GetMsgMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/mdata_' + str(maxLen) + '.npy')) \\\n",
        "            | (not os.path.exists(tempPath + '/mlabels_' + str(maxLen) + '.npy')):\n",
        "        np.save(tempPath + '/mdata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <GetMsgMapping> Save the mapped numpy data to ' + tempPath + '/mdata_' + str(maxLen) + '.npy.')\n",
        "        np.save(tempPath + '/mlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n",
        "        print('[INFO] <GetMsgMapping> Save the mapped numpy labels to ' + tempPath + '/mlabels_' + str(maxLen) + '.npy.')\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "class MsgRNN(nn.Module):\n",
        "    '''\n",
        "    MsgRNN : convert a commit message into a predicted label.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, preWeights, hiddenSize=32, hiddenLayers=1):\n",
        "        '''\n",
        "        define each layer in the network model.\n",
        "        :param preWeights: tensor pre-trained weights for embedding layer.\n",
        "        :param hiddenSize: node number in the hidden layer.\n",
        "        :param hiddenLayers: number of hidden layer.\n",
        "        '''\n",
        "\n",
        "        super(MsgRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "        vocabSize, embedDim = preWeights.size()\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabSize, embedding_dim=embedDim)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        # LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_size=embedDim, hidden_size=hiddenSize, num_layers=hiddenLayers, bidirectional=True)\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(hiddenSize * hiddenLayers * 2, class_num)\n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convert inputs to predictions.\n",
        "        :param x: input tensor. dimension: batch_size * diff_length * 1.\n",
        "        :return: self.softmax(final_out) - predictions.\n",
        "        [[0.3, 0.7], [0.2, 0.8], ...]\n",
        "        '''\n",
        "\n",
        "        # x             batch_size * diff_length * 1\n",
        "        embeds = self.embedding(x)\n",
        "        # embeds        batch_size * diff_length * embedding_dim\n",
        "        inputs = embeds.permute(1, 0, 2)\n",
        "        # inputs        diff_length * batch_size * (embedding_dim + _DiffExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(inputs)\n",
        "        # lstm_out      diff_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        feature_map = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # feature_map   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        final_out = self.fc(feature_map)    # batch_size * class_num\n",
        "        return self.softmax(final_out)      # batch_size * class_num\n",
        "\n",
        "def MsgRNNTrain(dTrain, lTrain, dValid, lValid, preWeights, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
        "    '''\n",
        "    Train the MsgRNN model.\n",
        "    :param dTrain: training data. [[n, ...], ...]\n",
        "    :param lTrain: training label. [[n, ...], ...]\n",
        "    :param dValid: validation data. [[n, ...], ...]\n",
        "    :param lValid: validation label. [[n, ...], ...]\n",
        "    :param preWeights: pre-trained weights for embedding layer.\n",
        "    :param batchsize: number of samples in a batch.\n",
        "    :param learnRate: learning rate.\n",
        "    :param dTest: test data. [[n, ...], ...]\n",
        "    :param lTest: test label. [[n, ...], ...]\n",
        "    :return: model - the MsgRNN model.\n",
        "    '''\n",
        "\n",
        "    # get the mark of the test dataset.\n",
        "    if dTest is None: dTest = []\n",
        "    if lTest is None: lTest = []\n",
        "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    if (markTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "    if (markTest):\n",
        "        test = torchdata.TensorDataset(xTest, yTest)\n",
        "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights.\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(2):\n",
        "        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = MsgRNN(preWeights, hiddenSize=_MRnnHidSiz_, hiddenLayers=_MRnnHidLay_)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[INFO] <MsgRNNTrain> ModelType: MsgRNN, HiddenNodes: %d, HiddenLayers: %d.' % (_MRnnHidSiz_, _MRnnHidLay_))\n",
        "    print('[INFO] <MsgRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d.' % (batchsize, learnRate, _MRnnMaxEpoch_, _MRnnPerEpoch_))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(_MRnnMaxEpoch_):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # back propagation.\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                # data conversion.\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                # forward propagation.\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # testing phase.\n",
        "        if (markTest):\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with torch.no_grad():\n",
        "                for iter, (data, label) in enumerate(testloader):\n",
        "                    # data conversion.\n",
        "                    data = data.to(device)\n",
        "                    label = label.contiguous().view(-1)\n",
        "                    label = label.to(device)\n",
        "                    # forward propagation.\n",
        "                    yhat = model.forward(data)  # get output\n",
        "                    # statistic\n",
        "                    preds = yhat.max(1)[1]\n",
        "                    predictions.extend(preds.int().tolist())\n",
        "                    labels.extend(label.int().tolist())\n",
        "                    torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # test accuracy.\n",
        "            accTest = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # output information.\n",
        "        if (0 == (epoch + 1) % _MRnnPerEpoch_):\n",
        "            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
        "            if (markTest):\n",
        "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
        "            print(strAcc)\n",
        "        # save the best model.\n",
        "        if (accList[-1] > max(accList[0:-1])):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_MsgRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch >= _MRnnJudEpoch_) and (accList[-1] < min(accList[-1-_MRnnJudEpoch_:-1])):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_MsgRNN.pth'))\n",
        "    print('[INFO] <MsgRNNTrain> Finish training MsgRNN model. (Best model: ' + tempPath + '/model_MsgRNN.pth)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def MsgRNNTest(model, dTest, lTest, batchsize=64):\n",
        "    '''\n",
        "    Test the MsgRNN model.\n",
        "    :param model: deep learning model.\n",
        "    :param dTest: test data.\n",
        "    :param lTest: test label.\n",
        "    :param batchsize: number of samples in a batch\n",
        "    :return: predictions - predicted labels. [[0], [1], ...]\n",
        "             accuracy - the total test accuracy. numeric\n",
        "    '''\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # load the model of recurrent neural network.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # forward propagation.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "\n",
        "    return predictions, accuracy\n",
        "\n",
        "def demoPatch():\n",
        "    '''\n",
        "    demo program of using both commit message and diff code to identify patches.\n",
        "    '''\n",
        "\n",
        "    # load data.\n",
        "    if (not os.path.exists(tempPath + '/data.npy')): # | (not _DEBUG_)\n",
        "        dataLoaded = ReadData()\n",
        "    else:\n",
        "        dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n",
        "\n",
        "    # get the diff file properties.\n",
        "    if (not os.path.exists(tempPath + '/props.npy')):\n",
        "        diffProps = GetDiffProps(dataLoaded)\n",
        "    else:\n",
        "        diffProps = np.load(tempPath + '/props.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Load ' + str(len(diffProps)) + ' diff property data from ' + tempPath + '/props.npy.')\n",
        "    # only maintain the diff parts of the code.\n",
        "    diffProps = ProcessTokens(diffProps, dType=_DTYP_, cType=_CTYP_)\n",
        "    # normalize the tokens of identifiers, literals, and comments.\n",
        "    diffProps = AbstractTokens(diffProps, iType=_NIND_, lType=_NLIT_)\n",
        "    # get the diff token vocabulary.\n",
        "    diffVocab, diffMaxLen = GetDiffVocab(diffProps)\n",
        "    # get the max diff length.\n",
        "    diffMaxLen = _DiffMaxLen_ if (diffMaxLen > _DiffMaxLen_) else diffMaxLen\n",
        "    # get the diff token dictionary.\n",
        "    diffDict = GetDiffDict(diffVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    diffPreWeights = GetDiffEmbed(diffDict, _DiffEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    diffData, diffLabels = GetDiffMapping(diffProps, diffMaxLen, diffDict)\n",
        "    # change the tokentypes into one-hot vector.\n",
        "    diffData = UpdateTokenTypes(diffData)\n",
        "\n",
        "    # get the commit messages from data.\n",
        "    if (not os.path.exists(tempPath + '/msgs.npy')):\n",
        "        commitMsgs = GetCommitMsgs(dataLoaded)\n",
        "    else:\n",
        "        commitMsgs = np.load(tempPath + '/msgs.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetCommitMsg> Load ' + str(len(commitMsgs)) + ' commit messages from ' + tempPath + '/msgs.npy.')\n",
        "    # get the message token vocabulary.\n",
        "    msgVocab, msgMaxLen = GetMsgVocab(commitMsgs)\n",
        "    # get the max msg length.\n",
        "    msgMaxLen = _MsgMaxLen_ if (msgMaxLen > _MsgMaxLen_) else msgMaxLen\n",
        "    # get the msg token dictionary.\n",
        "    msgDict = GetMsgDict(msgVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    msgPreWeights = GetMsgEmbed(msgDict, _MsgEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    msgData, msgLabels = GetMsgMapping(commitMsgs, msgMaxLen, msgDict)\n",
        "\n",
        "    # combine the diff data with the message data.\n",
        "    data, label = CombinePropsMsgs(diffData, msgData, diffLabels, msgLabels)\n",
        "    # split data into rest/test dataset.\n",
        "    dataTrain, labelTrain, dataTest, labelTest = SplitData(data, label, 'test', rate=0.2)\n",
        "    print('[INFO] <main> Get ' + str(len(dataTrain)) + ' TRAIN data, ' + str(len(dataTest))\n",
        "          + ' TEST data. (Total: ' + str(len(dataTrain) + len(dataTest)) + ')')\n",
        "\n",
        "    # PatchRNNTrain\n",
        "    if (_MODEL_) & (os.path.exists(tempPath + '/model_PatchRNN.pth')):\n",
        "        preWDiff = torch.from_numpy(diffPreWeights)\n",
        "        preWMsg = torch.from_numpy(msgPreWeights)\n",
        "        model = PatchRNN(preWDiff, preWMsg, hidSizDiff=_DRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayDiff=_DRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n",
        "        model.load_state_dict(torch.load(tempPath + '/model_PatchRNN.pth'))\n",
        "    else:\n",
        "        model = PatchRNNTrain(dataTrain, labelTrain, dataTest, labelTest, preWDiff=diffPreWeights, preWMsg=msgPreWeights,\n",
        "                             batchsize=_PRnnBatchSz_, learnRate=_PRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n",
        "\n",
        "    # PatchRNNTest\n",
        "    predictions, accuracy = PatchRNNTest(model, dataTest, labelTest, batchsize=_PRnnBatchSz_)\n",
        "    _, confusion = OutputEval(predictions, labelTest, 'PatchRNN')\n",
        "\n",
        "    return\n",
        "\n",
        "def ProcessTokens(props, dType=1, cType=1):\n",
        "    '''\n",
        "    only maintain the diff parts of the code.\n",
        "    :param props: the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :param dType: 0 - maintain both diff code and context code.\n",
        "                  1 - only maintain diff code.\n",
        "    :param cType: 0 - maintain both the code and comments.\n",
        "                  1 - only maintain code and delete comments.\n",
        "    :return: props - the normalized features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    # process diff code.\n",
        "    if (1 == dType):\n",
        "        propsNew = []\n",
        "        for item in props:\n",
        "            # the number of tokens.\n",
        "            numTokens = len(item[1])\n",
        "            # item[0]: tokens, item[1]: tokenTypes, item[2]: diffTypes, item[3]: label.\n",
        "            tokens = [item[0][n] for n in range(numTokens) if (item[2][n])]\n",
        "            tokenTypes = [item[1][n] for n in range(numTokens) if (item[2][n])]\n",
        "            diffTypes = [item[2][n] for n in range(numTokens) if (item[2][n])]\n",
        "            label = item[3]\n",
        "            # reconstruct sample.\n",
        "            sample = [tokens, tokenTypes, diffTypes, label]\n",
        "            propsNew.append(sample)\n",
        "        props = propsNew\n",
        "        print('[INFO] <ProcessTokens> Only maintain the diff parts of the code.')\n",
        "\n",
        "    # process comments.\n",
        "    if (1 == cType):\n",
        "        propsNew = []\n",
        "        for item in props:\n",
        "            # the number of tokens.\n",
        "            numTokens = len(item[1])\n",
        "            # item[0]: tokens, item[1]: tokenTypes, item[2]: diffTypes, item[3]: label.\n",
        "            tokens = [item[0][n] for n in range(numTokens) if (item[1][n] < 5)]\n",
        "            tokenTypes = [item[1][n] for n in range(numTokens) if (item[1][n] < 5)]\n",
        "            diffTypes = [item[2][n] for n in range(numTokens) if (item[1][n] < 5)]\n",
        "            label = item[3]\n",
        "            # reconstruct sample.\n",
        "            sample = [tokens, tokenTypes, diffTypes, label]\n",
        "            propsNew.append(sample)\n",
        "        props = propsNew\n",
        "        print('[INFO] <ProcessTokens> Delete the comment parts of the diff code.')\n",
        "\n",
        "    #print(props[0])\n",
        "\n",
        "    return props\n",
        "\n",
        "def AbstractTokens(props, iType=1, lType=1):\n",
        "    '''\n",
        "    abstract the tokens of identifiers, literals, and comments.\n",
        "    :param props: the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :param iType:   -1 - not abstract tokens.\n",
        "                     0 - only abstract variable type and function type. VAR / FUNC\n",
        "                     1 - abstract the identical variable names and function names.  VAR0, VAR1, ... / FUNC0, FUNC1, ...\n",
        "    :param lType:   -1 - not abstract tokens.\n",
        "                     0 - abstract literals with LITERAL.\n",
        "                     1 - abstract literals with LITERAL/n.\n",
        "    :return: props - the abstracted features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    if (iType not in [0, 1]) or (lType not in [0, 1]):\n",
        "        print('[INFO] <AbstractTokens> Not abstract the tokens of identifiers, literals, and comments.')\n",
        "        return props\n",
        "\n",
        "    for item in props:\n",
        "        # get tokens and token types.\n",
        "        tokens = item[0]\n",
        "        tokenTypes = item[1]\n",
        "        numTokens = len(tokenTypes)\n",
        "        #print(tokens)\n",
        "        #print(tokenTypes)\n",
        "        #print(numTokens)\n",
        "\n",
        "        # abstract literals and comments, and separate identifiers into variables and functions.\n",
        "        markVar = list(np.zeros(numTokens, dtype=int))\n",
        "        markFuc = list(np.zeros(numTokens, dtype=int))\n",
        "        for n in range(numTokens):\n",
        "            # 2: IDENTIFIER, 3: LITERAL, 5: COMMENT\n",
        "            if 5 == tokenTypes[n]:\n",
        "                tokens[n] = 'COMMENT'\n",
        "            elif 3 == tokenTypes[n]:\n",
        "                if (0 == lType):\n",
        "                    tokens[n] = 'LITERAL'\n",
        "                elif (1 == lType):\n",
        "                    if (not tokens[n].isdigit()):\n",
        "                        tokens[n] = 'LITERAL'\n",
        "            elif 2 == tokenTypes[n]:\n",
        "                # separate variable name and function name.\n",
        "                if (n < numTokens-1):\n",
        "                    if (tokens[n+1] == '('):\n",
        "                        markFuc[n] = 1\n",
        "                    else:\n",
        "                        markVar[n] = 1\n",
        "                else:\n",
        "                    markVar[n] = 1\n",
        "        #print(tokens)\n",
        "        #print(markVar)\n",
        "        #print(markFuc)\n",
        "\n",
        "        # abstract variables and functions.\n",
        "        if (0 == iType):\n",
        "            for n in range(numTokens):\n",
        "                if 1 == markVar[n]:\n",
        "                    tokens[n] = 'VAR'\n",
        "                elif 1 == markFuc[n]:\n",
        "                    tokens[n] = 'FUNC'\n",
        "        elif (1 == iType):\n",
        "            # get variable dictionary.\n",
        "            varList = [tokens[idx] for idx, mark in enumerate(markVar) if mark == 1]\n",
        "            varVoc  = {}.fromkeys(varList)\n",
        "            varVoc  = list(varVoc.keys())\n",
        "            varDict = {tk: 'VAR' + str(idx) for idx, tk in enumerate(varVoc)}\n",
        "            # get function dictionary.\n",
        "            fucList = [tokens[idx] for idx, mark in enumerate(markFuc) if mark == 1]\n",
        "            fucVoc  = {}.fromkeys(fucList)\n",
        "            fucVoc  = list(fucVoc.keys())\n",
        "            fucDict = {tk: 'FUNC' + str(idx) for idx, tk in enumerate(fucVoc)}\n",
        "            #print(varDict)\n",
        "            #print(fucDict)\n",
        "            for n in range(numTokens):\n",
        "                if 1 == markVar[n]:\n",
        "                    tokens[n] = varDict[tokens[n]]\n",
        "                elif 1 == markFuc[n]:\n",
        "                    tokens[n] = fucDict[tokens[n]]\n",
        "    #print(tokens)\n",
        "    print('[INFO] <AbstractTokens> Abstract the tokens of identifiers with iType ' + str(iType), end='')\n",
        "    print(' (VAR/FUNC).') if (0 == iType) else print(' (VARn/FUNCn).')\n",
        "    print('[INFO] <AbstractTokens> Abstract the tokens of literals, and comments with iType ' + str(lType), end='')\n",
        "    print(' (LITERAL/COMMENT).') if (0 == lType) else print(' (LITERAL/n/COMMENT).')\n",
        "\n",
        "    return props\n",
        "\n",
        "def CombinePropsMsgs(props, msgs, plabels, mlabels):\n",
        "    '''\n",
        "    Combine the diff props with the commit messages.\n",
        "    :param props: diff data. [[[n, {0~5}, {-1~1}], ...], ...] or [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param msgs: message data. [[n, ...], ...]\n",
        "    :param plabels: diff labels. [[0/1], ...]\n",
        "    :param mlabels: message labels. [[0/1], ...]\n",
        "    :return: np.array(data) - combined data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}, n], ...], ...]\n",
        "             np.array(plabels) - combined labels. [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    # check the lengths.\n",
        "    if (len(plabels) != len(mlabels)):\n",
        "        print('[ERROR] <CombinePropsMsgs> the data lengths are mismatch.')\n",
        "        return [[]], [[]]\n",
        "\n",
        "    # check the labels.\n",
        "    cntMatch = 0\n",
        "    for n in range(len(plabels)):\n",
        "        if (plabels[n][0] == mlabels[n][0]):\n",
        "            cntMatch += 1\n",
        "    if (cntMatch != len(plabels)):\n",
        "        print('[ERROR] <CombinePropsMsgs> the labels are mismatch. ' + str(cntMatch) + '/' + str(len(plabels)) + '.')\n",
        "        return [[]], [[]]\n",
        "\n",
        "    #print(props[1], len(props[1]))\n",
        "    #print(msgs[1], len(msgs[1]))\n",
        "\n",
        "    data = []\n",
        "    for n in range(len(plabels)):\n",
        "        # get the diff prop and message.\n",
        "        prop = props[n]\n",
        "        msg = msgs[n]\n",
        "        # pad data.\n",
        "        if (_DiffMaxLen_ >= _MsgMaxLen_):\n",
        "            msg = np.pad(msg, (0, _DiffMaxLen_ - _MsgMaxLen_), 'constant')\n",
        "        else:\n",
        "            prop = np.pad(prop, ((0, _MsgMaxLen_ - _DiffMaxLen_), (0, 0)), 'constant')\n",
        "        #print(msg, len(msg))\n",
        "        #print(prop, len(prop))\n",
        "        # reconstruct sample.\n",
        "        sample = np.vstack((prop.T, msg))\n",
        "        # append the sample to data.\n",
        "        data.append(sample.T)\n",
        "\n",
        "    #print(np.array(data[1]), len(data[1]))\n",
        "    print('[INFO] <CombinePropsMsgs> Combine the diff props with the commit messages.')\n",
        "\n",
        "    return np.array(data), np.array(plabels)\n",
        "\n",
        "class PatchRNN(nn.Module):\n",
        "    '''\n",
        "    PatchRNN : convert a patch data into a predicted label.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, preWDiff, preWMsg, hidSizDiff=32, hidSizMsg=32, hidLayDiff=1, hidLayMsg=1):\n",
        "        '''\n",
        "        define each layer in the network model.\n",
        "        :param preWDiff: tensor pre-trained weights for embedding layer for diff.\n",
        "        :param preWMsg: tensor pre-trained weights for embedding layer for msg.\n",
        "        :param hidSizDiff: node number in the hidden layer for diff.\n",
        "        :param hidSizMsg: node number in the hidden layer for msg.\n",
        "        :param hidLayDiff: number of hidden layer for diff.\n",
        "        :param hidLayMsg: number of hidden layer for msg.\n",
        "        '''\n",
        "\n",
        "        super(PatchRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "    # diff.\n",
        "        vSizDiff, emDimDiff = preWDiff.size()\n",
        "        # Embedding Layer for diff.\n",
        "        self.embedDiff = nn.Embedding(num_embeddings=vSizDiff, embedding_dim=emDimDiff)\n",
        "        self.embedDiff.load_state_dict({'weight': preWDiff})\n",
        "        self.embedDiff.weight.requires_grad = True\n",
        "        # LSTM Layer for diff.\n",
        "        if _DEBUG_: print(_DiffExtraDim_)\n",
        "        self.lstmDiff = nn.LSTM(input_size=emDimDiff+_DiffExtraDim_, hidden_size=hidSizDiff, num_layers=hidLayDiff, bidirectional=True)\n",
        "        # Fully-Connected Layer for diff.\n",
        "        self.fcDiff = nn.Linear(hidSizDiff * hidLayDiff * 2, hidSizDiff * hidLayDiff)\n",
        "    # msg.\n",
        "        vSizMsg, emDimMsg = preWMsg.size()\n",
        "        # Embedding Layer for msg.\n",
        "        self.embedMsg = nn.Embedding(num_embeddings=vSizMsg, embedding_dim=emDimMsg)\n",
        "        self.embedMsg.load_state_dict({'weight': preWMsg})\n",
        "        self.embedMsg.weight.requires_grad = True\n",
        "        # LSTM Layer for msg.\n",
        "        self.lstmMsg = nn.LSTM(input_size=emDimMsg, hidden_size=hidSizMsg, num_layers=hidLayMsg, bidirectional=True)\n",
        "        # Fully-Connected Layer for msg.\n",
        "        self.fcMsg = nn.Linear(hidSizMsg * hidLayMsg * 2, hidSizMsg * hidLayMsg)\n",
        "    # common.\n",
        "        # Fully-Connected Layer.\n",
        "        self.fc = nn.Linear((hidSizDiff * hidLayDiff + hidSizMsg * hidLayMsg) * 2, class_num)\n",
        "        self.fc1 = nn.Linear((hidSizDiff * hidLayDiff + hidSizMsg * hidLayMsg) * 2, hidSizDiff * hidLayDiff + hidSizMsg * hidLayMsg)\n",
        "        self.fc2 = nn.Linear(hidSizDiff * hidLayDiff + hidSizMsg * hidLayMsg, class_num)\n",
        "        # Softmax non-linearity.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convert inputs to predictions.\n",
        "        :param x: input tensor. dimension: batch_size * diff_length * feature_dim.\n",
        "        :return: self.softmax(final_out) - predictions.\n",
        "        [[0.3, 0.7], [0.2, 0.8], ...]\n",
        "        '''\n",
        "\n",
        "    # diff.\n",
        "        xDiff = x[:, :_DiffMaxLen_, :-1]\n",
        "        # xDiff         batch_size * diff_length * feature_dim\n",
        "        # print(xDiff.size())\n",
        "        embedsDiff = self.embedDiff(xDiff[:, :, 0])\n",
        "        # embedsDiff    batch_size * diff_length * embed_dim_diff\n",
        "        features = xDiff[:, :, 1:]\n",
        "        # features      batch_size * diff_length * _DiffExtraDim_\n",
        "        inputsDiff = torch.cat((embedsDiff.float(), features.float()), 2)\n",
        "        # inputsDiff    batch_size * diff_length * (embed_dim_diff + _DiffExtraDim_)\n",
        "        inputsDiff = inputsDiff.permute(1, 0, 2)\n",
        "        # inputsDiff    diff_length * batch_size * (embed_dim_diff + _DiffExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstmDiff(inputsDiff)\n",
        "        # lstm_out      diff_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        featMapDiff = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # featMapDiff   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        #print(featMapDiff.size())\n",
        "    # msg.\n",
        "        xMsg = x[:, :_MsgMaxLen_, -1]\n",
        "        # xMsg          batch_size * msg_length * 1\n",
        "        # print(xMsg.size())\n",
        "        embedsMsg = self.embedMsg(xMsg)\n",
        "        # embedsMsg     batch_size * diff_length * embed_dim_msg\n",
        "        inputsMsg = embedsMsg.permute(1, 0, 2)\n",
        "        # inputsMsg     diff_length * batch_size * (embed_dim_msg + _DiffExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstmMsg(inputsMsg)\n",
        "        # lstm_out      diff_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        featMapMsg = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # featMapMsg    batch_size * (hidden_size * num_layers * direction_num)\n",
        "        #print(featMapMsg.size())\n",
        "    # common.\n",
        "        # combine + 1 layer.\n",
        "        featMap = torch.cat((featMapDiff, featMapMsg), dim=1)\n",
        "        #print(featMap.size())\n",
        "        #final_out = self.fc(featMap)        # batch_size * class_num\n",
        "        # combine + 2 layers.\n",
        "        featMap = self.fc1(featMap)\n",
        "        final_out = self.fc2(featMap)\n",
        "        # separate + 2 layers.\n",
        "        #featMapDiff = self.fcDiff(featMapDiff)\n",
        "        #featMapMsg = self.fcMsg(featMapMsg)\n",
        "        #featMap = torch.cat((featMapDiff, featMapMsg), dim=1)\n",
        "        #final_out = self.fc2(featMap)\n",
        "        #print(final_out.size())\n",
        "        return self.softmax(final_out)      # batch_size * class_num\n",
        "\n",
        "def PatchRNNTrain(dTrain, lTrain, dValid, lValid, preWDiff, preWMsg, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
        "    '''\n",
        "    Train the PatchRNN model.\n",
        "    :param dTrain: training data. [[n, ...], ...]\n",
        "    :param lTrain: training label. [[n, ...], ...]\n",
        "    :param dValid: validation data. [[n, ...], ...]\n",
        "    :param lValid: validation label. [[n, ...], ...]\n",
        "    :param preWDiff: pre-trained weights for diff embedding layer.\n",
        "    :param preWMsg: pre-trained weights for msg embedding layer.\n",
        "    :param batchsize: number of samples in a batch.\n",
        "    :param learnRate: learning rate.\n",
        "    :param dTest: test data. [[n, ...], ...]\n",
        "    :param lTest: test label. [[n, ...], ...]\n",
        "    :return: model - the PatchRNN model.\n",
        "    '''\n",
        "\n",
        "    # get the mark of the test dataset.\n",
        "    if dTest is None: dTest = []\n",
        "    if lTest is None: lTest = []\n",
        "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    if (markTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "    if (markTest):\n",
        "        test = torchdata.TensorDataset(xTest, yTest)\n",
        "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights.\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(2):\n",
        "        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWDiff = torch.from_numpy(preWDiff)\n",
        "    preWMsg = torch.from_numpy(preWMsg)\n",
        "    model = PatchRNN(preWDiff, preWMsg, hidSizDiff=_DRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayDiff=_DRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[INFO] <PatchRNNTrain> ModelType: PatchRNN.')\n",
        "    print('[INFO] <PatchRNNTrain> Diff Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_DiffEmbedDim_, _DiffMaxLen_, _DRnnHidSiz_, _DRnnHidLay_))\n",
        "    print('[INFO] <PatchRNNTrain> Msg  Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_MsgEmbedDim_, _MsgMaxLen_, _MRnnHidSiz_, _MRnnHidLay_))\n",
        "    print('[INFO] <PatchRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _PRnnMaxEpoch_, _PRnnPerEpoch_, _PRnnJudEpoch_))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(_PRnnMaxEpoch_):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # back propagation.\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                # data conversion.\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                # forward propagation.\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # testing phase.\n",
        "        if (markTest):\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with torch.no_grad():\n",
        "                for iter, (data, label) in enumerate(testloader):\n",
        "                    # data conversion.\n",
        "                    data = data.to(device)\n",
        "                    label = label.contiguous().view(-1)\n",
        "                    label = label.to(device)\n",
        "                    # forward propagation.\n",
        "                    yhat = model.forward(data)  # get output\n",
        "                    # statistic\n",
        "                    preds = yhat.max(1)[1]\n",
        "                    predictions.extend(preds.int().tolist())\n",
        "                    labels.extend(label.int().tolist())\n",
        "                    torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # test accuracy.\n",
        "            accTest = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # output information.\n",
        "        if (0 == (epoch + 1) % _PRnnPerEpoch_):\n",
        "            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
        "            if (markTest):\n",
        "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
        "            print(strAcc)\n",
        "        # save the best model.\n",
        "        if (accList[-1] > max(accList[0:-1])):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_PatchRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch >= _PRnnJudEpoch_) and (accList[-1] < min(accList[-1-_PRnnJudEpoch_:-1])):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_PatchRNN.pth'))\n",
        "    print('[INFO] <PatchRNNTrain> Finish training PatchRNN model. (Best model: ' + tempPath + '/model_PatchRNN.pth)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def PatchRNNTest(model, dTest, lTest, batchsize=64):\n",
        "    '''\n",
        "    Test the PatchRNN model.\n",
        "    :param model: deep learning model.\n",
        "    :param dTest: test data.\n",
        "    :param lTest: test label.\n",
        "    :param batchsize: number of samples in a batch\n",
        "    :return: predictions - predicted labels. [[0], [1], ...]\n",
        "             accuracy - the total test accuracy. numeric\n",
        "    '''\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # load the model of recurrent neural network.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # forward propagation.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "\n",
        "    return predictions, accuracy\n",
        "\n",
        "def demoTwin():\n",
        "    '''\n",
        "        demo program of using both commit message and diff code to identify patches.\n",
        "        '''\n",
        "\n",
        "    # load data.\n",
        "    if (not os.path.exists(tempPath + '/data.npy')):  # | (not _DEBUG_)\n",
        "        dataLoaded = ReadData()\n",
        "    else:\n",
        "        dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n",
        "\n",
        "    # get the diff file properties.\n",
        "    if (not os.path.exists(tempPath + '/props.npy')):\n",
        "        diffProps = GetDiffProps(dataLoaded)\n",
        "    else:\n",
        "        diffProps = np.load(tempPath + '/props.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Load ' + str(len(diffProps)) + ' diff property data from ' + tempPath + '/props.npy.')\n",
        "    # maintain both the context and diff parts. Delete comments.\n",
        "    diffProps = ProcessTokens(diffProps, dType=0, cType=_CTYP_)\n",
        "    # normalize the tokens of identifiers (VARn/FUNCn), literals (LITERAL/n), and comments (none).\n",
        "    diffProps = AbstractTokens(diffProps, iType=_NIND_, lType=_NLIT_)\n",
        "    # get the diff token vocabulary.\n",
        "    diffVocab, _ = GetDiffVocab(diffProps)\n",
        "    # get the diff token dictionary.\n",
        "    diffDict = GetDiffDict(diffVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    twinPreWeights = GetDiffEmbed(diffDict, _TwinEmbedDim_)\n",
        "    # divide diff code into before/after-version code.\n",
        "    twinProps, twinMaxLen = DivideBeforeAfter(diffProps)\n",
        "    # get the max twin length.\n",
        "    twinMaxLen = _TwinMaxLen_ if (twinMaxLen > _DiffMaxLen_) else twinMaxLen\n",
        "    # get the mapping for feature data and labels.\n",
        "    twinData, twinLabels = GetTwinMapping(twinProps, twinMaxLen, diffDict)\n",
        "    # change the tokentypes into one-hot vector.\n",
        "    twinData = UpdateTwinTokenTypes(twinData)\n",
        "\n",
        "    # get the commit messages from data.\n",
        "    if (not os.path.exists(tempPath + '/msgs.npy')):\n",
        "        commitMsgs = GetCommitMsgs(dataLoaded)\n",
        "    else:\n",
        "        commitMsgs = np.load(tempPath + '/msgs.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetCommitMsg> Load ' + str(len(commitMsgs)) + ' commit messages from ' + tempPath + '/msgs.npy.')\n",
        "    # get the message token vocabulary.\n",
        "    msgVocab, msgMaxLen = GetMsgVocab(commitMsgs)\n",
        "    # get the max msg length.\n",
        "    msgMaxLen = _MsgMaxLen_ if (msgMaxLen > _MsgMaxLen_) else msgMaxLen\n",
        "    # get the msg token dictionary.\n",
        "    msgDict = GetMsgDict(msgVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    msgPreWeights = GetMsgEmbed(msgDict, _MsgEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    msgData, msgLabels = GetMsgMapping(commitMsgs, msgMaxLen, msgDict)\n",
        "\n",
        "    # combine the twin data with the message data.\n",
        "    data, label = CombineTwinMsgs(twinData, msgData, twinLabels, msgLabels)\n",
        "    # split data into rest/test dataset.\n",
        "    dataTrain, labelTrain, dataTest, labelTest = SplitData(data, label, 'test', rate=0.2)\n",
        "    print('[INFO] <demoTwin> Get ' + str(len(dataTrain)) + ' TRAIN data, ' + str(len(dataTest))\n",
        "          + ' TEST data. (Total: ' + str(len(dataTrain) + len(dataTest)) + ')')\n",
        "\n",
        "    # TwinRNNTrain\n",
        "    if (_MODEL_) & (os.path.exists(tempPath + '/model_TwinRNN.pth')):\n",
        "        preWTwin = torch.from_numpy(twinPreWeights)\n",
        "        preWMsg = torch.from_numpy(msgPreWeights)\n",
        "        model = TwinRNN(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n",
        "        model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n",
        "    else:\n",
        "        model = TwinRNNTrain(dataTrain, labelTrain, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n",
        "                             batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n",
        "\n",
        "    # TwinRNNTest\n",
        "    predictions, accuracy = TwinRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n",
        "    _, confusion = OutputEval(predictions, labelTest, 'TwinRNN')\n",
        "\n",
        "    return\n",
        "\n",
        "def DivideBeforeAfter(diffProps):\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'twinlen.csv', 'w')\n",
        "\n",
        "    twinProps = []\n",
        "    maxLen = 0\n",
        "    # for each sample in diffProps.\n",
        "    for item in diffProps:\n",
        "        # get the tk, tkT, dfT, lb.\n",
        "        tokens = item[0]\n",
        "        tokenTypes = item[1]\n",
        "        diffTypes = item[2]\n",
        "        label = item[3]\n",
        "        numTokens = len(diffTypes)\n",
        "        # reconstruct tkB, tkTB, tkA, tkTA.\n",
        "        tokensB = [tokens[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n",
        "        tokenTypesB = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n",
        "        tokensA = [tokens[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n",
        "        tokenTypesA = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n",
        "        # reconstruct new sample.\n",
        "        sample = [tokensB, tokenTypesB, tokensA, tokenTypesA, label]\n",
        "        twinProps.append(sample)\n",
        "        # get max length.\n",
        "        maxLenAB = max(len(tokenTypesB), len(tokenTypesA))\n",
        "        maxLen = maxLenAB if (maxLen < maxLenAB) else maxLen\n",
        "        fp.write(str(len(tokenTypesB)) + '\\n')\n",
        "        fp.write(str(len(tokenTypesA)) + '\\n')\n",
        "    fp.close()\n",
        "\n",
        "    #print(twinProps[0])\n",
        "    #print(maxLen)\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <DivideBeforeAfter> Divide diff code into BEFORE-version and AFTER-version code.')\n",
        "    print('[INFO] <DivideBeforeAfter> The max length in BEFORE/AFTER-version code is ' + str(maxLen) + ' tokens. (hyperparameter: _TwinMaxLen_ = ' + str(_TwinMaxLen_) + ')')\n",
        "\n",
        "    return twinProps, maxLen\n",
        "\n",
        "def GetTwinMapping(props, maxLen, tokenDict):\n",
        "    '''\n",
        "    Map the feature data into indexed data.\n",
        "    :param props: the features of diff code.\n",
        "    [[[tokens], [nums], [tokens], [nums], 0/1], ...]\n",
        "    :param maxLen: the max length of a twin code.\n",
        "    :param tokenDict: the dictionary of diff vocabulary.\n",
        "    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n",
        "    :return: np.array(data) - feature data.\n",
        "             [[[n, {0~5}, n, {0~5}], ...], ...]\n",
        "             np.array(labels) - labels.\n",
        "             [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def PadList(dList, pad, length):\n",
        "        '''\n",
        "        Pad the list data to a fixed length.\n",
        "        :param dList: the list data - [ , , ...]\n",
        "        :param pad: the variable used to pad.\n",
        "        :param length: the fixed length.\n",
        "        :return: dList - padded list data. [ , , ...]\n",
        "        '''\n",
        "\n",
        "        if len(dList) <= length:\n",
        "            dList.extend(pad for i in range(length - len(dList)))\n",
        "        elif len(dList) > length:\n",
        "            dList = dList[0:length]\n",
        "\n",
        "        return dList\n",
        "\n",
        "    # initialize the data and labels.\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # for each sample.\n",
        "    for item in props:\n",
        "        # initialize sample.\n",
        "        sample = []\n",
        "\n",
        "        # process tokensB.\n",
        "        tokens = item[0]\n",
        "        tokens = PadList(tokens, '<pad>', maxLen)\n",
        "        tokens2index = []\n",
        "        for tk in tokens:\n",
        "            tokens2index.append(tokenDict[tk])\n",
        "        sample.append(tokens2index)\n",
        "        # process tokenTypesB.\n",
        "        tokenTypes = item[1]\n",
        "        tokenTypes = PadList(tokenTypes, 0, maxLen)\n",
        "        sample.append(tokenTypes)\n",
        "        # process tokensA.\n",
        "        tokens = item[2]\n",
        "        tokens = PadList(tokens, '<pad>', maxLen)\n",
        "        tokens2index = []\n",
        "        for tk in tokens:\n",
        "            tokens2index.append(tokenDict[tk])\n",
        "        sample.append(tokens2index)\n",
        "        # process tokenTypesA.\n",
        "        tokenTypes = item[3]\n",
        "        tokenTypes = PadList(tokenTypes, 0, maxLen)\n",
        "        sample.append(tokenTypes)\n",
        "\n",
        "        # process sample.\n",
        "        sample = np.array(sample).T\n",
        "        data.append(sample)\n",
        "        # process label.\n",
        "        label = item[4]\n",
        "        labels.append([label])\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] data:')\n",
        "        print(data[0:3])\n",
        "        print('[DEBUG] labels:')\n",
        "        print(labels[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetTwinMapping> Create ' + str(len(data)) + ' feature data with ' + str(len(data[0])) + ' * ' + str(len(data[0][0])) + ' matrix.')\n",
        "    print('[INFO] <GetTwinMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/tdata_' + str(maxLen) + '.npy')) \\\n",
        "            | (not os.path.exists(tempPath + '/tlabels_' + str(maxLen) + '.npy')):\n",
        "        np.save(tempPath + '/tdata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <GetTwinMapping> Save the mapped numpy data to ' + tempPath + '/tdata_' + str(maxLen) + '.npy.')\n",
        "        np.save(tempPath + '/tlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n",
        "        print('[INFO] <GetTwinMapping> Save the mapped numpy labels to ' + tempPath + '/tlabels_' + str(maxLen) + '.npy.')\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def UpdateTwinTokenTypes(data):\n",
        "    '''\n",
        "    Update the token type in the feature data into one-hot vector.\n",
        "    :param data: feature data. [[[n, {0~5}, n, {0~5},], ...], ...]\n",
        "    :return: np.array(newData). [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1], ...], ...]\n",
        "    '''\n",
        "\n",
        "    newData = []\n",
        "    # for each sample.\n",
        "    for item in data:\n",
        "        # get the transpose of props.\n",
        "        itemT = item.T\n",
        "        # initialize new sample.\n",
        "        newItem = []\n",
        "        newItem.append(itemT[0])\n",
        "        newItem.extend(np.zeros((5, len(item)), dtype=int))\n",
        "        newItem.append(itemT[2])\n",
        "        newItem.extend(np.zeros((5, len(item)), dtype=int))\n",
        "        # assign the new sample.\n",
        "        for i in range(len(item)):\n",
        "            tokenType = itemT[1][i]\n",
        "            if (tokenType):\n",
        "                newItem[tokenType][i] = 1\n",
        "            tokenType = itemT[3][i]\n",
        "            if (tokenType):\n",
        "                newItem[tokenType+6][i] = 1\n",
        "        # get the transpose of new sample.\n",
        "        newItem = np.array(newItem).T\n",
        "        # append new sample.\n",
        "        newData.append(newItem)\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] newData:')\n",
        "        print(newData[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <UpdateTwinTokenTypes> Update ' + str(len(newData)) + ' feature data with ' + str(len(newData[0])) + ' * ' + str(len(newData[0][0])) + ' matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/newtdata_' + str(len(newData[0])) + '.npy')):\n",
        "        np.save(tempPath + '/newtdata_' + str(len(newData[0])) + '.npy', newData, allow_pickle=True)\n",
        "        print('[INFO] <UpdateTwinTokenTypes> Save the mapped numpy data to ' + tempPath + '/newtdata_' + str(len(newData[0])) + '.npy.')\n",
        "\n",
        "    # change marco.\n",
        "    global _TwinExtraDim_\n",
        "    _TwinExtraDim_ = 5\n",
        "\n",
        "    return np.array(newData)\n",
        "\n",
        "def CombineTwinMsgs(props, msgs, plabels, mlabels):\n",
        "    '''\n",
        "    Combine the twin props with the commit messages.\n",
        "    :param props: twin data. [[[n, {0~5}, n, {0~5}], ...], ...] or [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1], ...], ...]\n",
        "    :param msgs: message data. [[n, ...], ...]\n",
        "    :param plabels: twin labels. [[0/1], ...]\n",
        "    :param mlabels: message labels. [[0/1], ...]\n",
        "    :return: np.array(data) - combined data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1, n], ...], ...]\n",
        "             np.array(plabels) - combined labels. [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    # check the lengths.\n",
        "    if (len(plabels) != len(mlabels)):\n",
        "        print('[ERROR] <CombineTwinMsgs> the data lengths are mismatch.')\n",
        "        return [[]], [[]]\n",
        "\n",
        "    # check the labels.\n",
        "    cntMatch = 0\n",
        "    for n in range(len(plabels)):\n",
        "        if (plabels[n][0] == mlabels[n][0]):\n",
        "            cntMatch += 1\n",
        "    if (cntMatch != len(plabels)):\n",
        "        print('[ERROR] <CombineTwinMsgs> the labels are mismatch. ' + str(cntMatch) + '/' + str(len(plabels)) + '.')\n",
        "        return [[]], [[]]\n",
        "\n",
        "    #print(props[1], len(props[1]))\n",
        "    #print(msgs[1], len(msgs[1]))\n",
        "\n",
        "    data = []\n",
        "    for n in range(len(plabels)):\n",
        "        # get the twin prop and message.\n",
        "        prop = props[n]\n",
        "        msg = msgs[n]\n",
        "        # pad data.\n",
        "        if (_TwinMaxLen_ >= _MsgMaxLen_):\n",
        "            msg = np.pad(msg, (0, _TwinMaxLen_ - _MsgMaxLen_), 'constant')\n",
        "        else:\n",
        "            prop = np.pad(prop, ((0, _MsgMaxLen_ - _TwinMaxLen_), (0, 0)), 'constant')\n",
        "        #print(msg, len(msg))\n",
        "        #print(prop, len(prop))\n",
        "        # reconstruct sample.\n",
        "        sample = np.vstack((prop.T, msg))\n",
        "        # append the sample to data.\n",
        "        data.append(sample.T)\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print(np.array(data[0:3]))\n",
        "\n",
        "    print('[INFO] <CombineTwinMsgs> Combine the twin props with the commit messages.')\n",
        "\n",
        "    return np.array(data), np.array(plabels)\n",
        "\n",
        "class TwinRNN(nn.Module):\n",
        "    '''\n",
        "    TwinRNN : convert a patch data into a predicted label.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, preWTwin, preWMsg, hidSizTwin=32, hidSizMsg=32, hidLayTwin=1, hidLayMsg=1):\n",
        "        '''\n",
        "        define each layer in the network model.\n",
        "        :param preWTwin: tensor pre-trained weights for embedding layer for twin.\n",
        "        :param preWMsg: tensor pre-trained weights for embedding layer for msg.\n",
        "        :param hidSizTwin: node number in the hidden layer for twin.\n",
        "        :param hidSizMsg: node number in the hidden layer for msg.\n",
        "        :param hidLayTwin: number of hidden layer for twin.\n",
        "        :param hidLayMsg: number of hidden layer for msg.\n",
        "        '''\n",
        "\n",
        "        super(TwinRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "    # twin.\n",
        "        vSizTwin, emDimTwin = preWTwin.size()\n",
        "        # Embedding Layer for twin.\n",
        "        self.embedTwin = nn.Embedding(num_embeddings=vSizTwin, embedding_dim=emDimTwin)\n",
        "        self.embedTwin.load_state_dict({'weight': preWTwin})\n",
        "        self.embedTwin.weight.requires_grad = True\n",
        "        # LSTM Layer for twin.\n",
        "        if _DEBUG_: print(_TwinExtraDim_)\n",
        "        self.lstmTwin = nn.LSTM(input_size=emDimTwin+_TwinExtraDim_, hidden_size=hidSizTwin, num_layers=hidLayTwin, bidirectional=True)\n",
        "    # msg.\n",
        "        vSizMsg, emDimMsg = preWMsg.size()\n",
        "        # Embedding Layer for msg.\n",
        "        self.embedMsg = nn.Embedding(num_embeddings=vSizMsg, embedding_dim=emDimMsg)\n",
        "        self.embedMsg.load_state_dict({'weight': preWMsg})\n",
        "        self.embedMsg.weight.requires_grad = True\n",
        "        # LSTM Layer for msg.\n",
        "        self.lstmMsg = nn.LSTM(input_size=emDimMsg, hidden_size=hidSizMsg, num_layers=hidLayMsg, bidirectional=True)\n",
        "    # common.\n",
        "        # Fully-Connected Layer.\n",
        "        self.fc1 = nn.Linear(hidSizTwin * hidLayTwin * 4, hidSizTwin * hidLayTwin * 2)\n",
        "        self.fc2 = nn.Linear(hidSizTwin * hidLayTwin * 2, class_num)\n",
        "        self.fc3 = nn.Linear((hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg) * 2, hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg)\n",
        "        self.fc4 = nn.Linear(hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg, class_num)\n",
        "        # Softmax non-linearity.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convert inputs to predictions.\n",
        "        :param x: input tensor. dimension: batch_size * twin_length * feature_dim.\n",
        "        :return: self.softmax(final_out) - predictions.\n",
        "        [[0.3, 0.7], [0.2, 0.8], ...]\n",
        "        '''\n",
        "\n",
        "    # twin 1.\n",
        "        xTwin = x[:, :_TwinMaxLen_, :6]\n",
        "        # xTwin         batch_size * twin_length * feature_dim\n",
        "        #print(xTwin.size())\n",
        "        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n",
        "        # embedsTwin    batch_size * twin_length * embed_dim_twin\n",
        "        features = xTwin[:, :, 1:]\n",
        "        # features      batch_size * twin_length * _TwinExtraDim_\n",
        "        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2)\n",
        "        #print(inputsTwin.size())\n",
        "        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n",
        "        inputsTwin = inputsTwin.permute(1, 0, 2)\n",
        "        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n",
        "        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        featMapTwin1 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # featMapTwin1   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        #print(featMapTwin1)\n",
        "    # twin 2.\n",
        "        xTwin = x[:, :_TwinMaxLen_, 6:-1]\n",
        "        # xTwin         batch_size * twin_length * feature_dim\n",
        "        #print(xTwin.size())\n",
        "        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n",
        "        # embedsTwin    batch_size * twin_length * embed_dim_twin\n",
        "        features = xTwin[:, :, 1:]\n",
        "        # features      batch_size * twin_length * _TwinExtraDim_\n",
        "        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2)\n",
        "        #print(inputsTwin.size())\n",
        "        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n",
        "        inputsTwin = inputsTwin.permute(1, 0, 2)\n",
        "        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n",
        "        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        featMapTwin2 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # featMapTwin2   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        #print(featMapTwin2)\n",
        "    # msg.\n",
        "        xMsg = x[:, :_MsgMaxLen_, -1]\n",
        "        # xMsg          batch_size * msg_length * 1\n",
        "        # print(xMsg.size())\n",
        "        embedsMsg = self.embedMsg(xMsg)\n",
        "        # embedsMsg     batch_size * msg_length * embed_dim_msg\n",
        "        inputsMsg = embedsMsg.permute(1, 0, 2)\n",
        "        # inputsMsg     msg_length * batch_size * (embed_dim_msg)\n",
        "        lstm_out, (h_n, c_n) = self.lstmMsg(inputsMsg)\n",
        "        # lstm_out      msg_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        featMapMsg = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # featMapMsg    batch_size * (hidden_size * num_layers * direction_num)\n",
        "        #print(featMapMsg.size())\n",
        "    # common.\n",
        "        # combine twins.\n",
        "        featMap = torch.cat((featMapTwin1, featMapTwin2), dim=1)\n",
        "        # fc layers.\n",
        "        featMap = self.fc1(featMap)\n",
        "        if (0 == _TWIN_): # (only twins).\n",
        "            final_out = self.fc2(featMap)\n",
        "        elif (1 == _TWIN_): # (twins + msg).\n",
        "            # combine twins + msg.\n",
        "            featMap = torch.cat((featMap, featMapMsg), dim=1)\n",
        "            # fc 2 layers.\n",
        "            featMap = self.fc3(featMap)\n",
        "            final_out = self.fc4(featMap)\n",
        "        #print(final_out.size())\n",
        "        return self.softmax(final_out)      # batch_size * class_num\n",
        "\n",
        "def TwinRNNTrain(dTrain, lTrain, dValid, lValid, preWTwin, preWMsg, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
        "    '''\n",
        "    Train the TwinRNN model.\n",
        "    :param dTrain: training data. [[n, ...], ...]\n",
        "    :param lTrain: training label. [[n, ...], ...]\n",
        "    :param dValid: validation data. [[n, ...], ...]\n",
        "    :param lValid: validation label. [[n, ...], ...]\n",
        "    :param preWDiff: pre-trained weights for diff embedding layer.\n",
        "    :param preWMsg: pre-trained weights for msg embedding layer.\n",
        "    :param batchsize: number of samples in a batch.\n",
        "    :param learnRate: learning rate.\n",
        "    :param dTest: test data. [[n, ...], ...]\n",
        "    :param lTest: test label. [[n, ...], ...]\n",
        "    :return: model - the TwinRNN model.\n",
        "    '''\n",
        "\n",
        "    # get the mark of the test dataset.\n",
        "    if dTest is None: dTest = []\n",
        "    if lTest is None: lTest = []\n",
        "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    if (markTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "    if (markTest):\n",
        "        test = torchdata.TensorDataset(xTest, yTest)\n",
        "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # get training weights.\n",
        "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
        "    weights = []\n",
        "    for lb in range(2):\n",
        "        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n",
        "    lbWeights = torch.FloatTensor(weights).cuda()\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWTwin = torch.from_numpy(preWTwin)\n",
        "    preWMsg = torch.from_numpy(preWMsg)\n",
        "    model = TwinRNN(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[INFO] <TwinRNNTrain> ModelType: TwinRNN.')\n",
        "    print('[INFO] <TwinRNNTrain> Code Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_TwinEmbedDim_, _TwinMaxLen_, _TRnnHidSiz_, _TRnnHidLay_))\n",
        "    print('[INFO] <TwinRNNTrain> Msg  Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_MsgEmbedDim_, _MsgMaxLen_, _MRnnHidSiz_, _MRnnHidLay_))\n",
        "    print('[INFO] <TwinRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_, _TRnnJudEpoch_))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(_TRnnMaxEpoch_):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # back propagation.\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                # data conversion.\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                # forward propagation.\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        # testing phase.\n",
        "        if (markTest):\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with torch.no_grad():\n",
        "                for iter, (data, label) in enumerate(testloader):\n",
        "                    # data conversion.\n",
        "                    data = data.to(device)\n",
        "                    label = label.contiguous().view(-1)\n",
        "                    label = label.to(device)\n",
        "                    # forward propagation.\n",
        "                    yhat = model.forward(data)  # get output\n",
        "                    # statistic\n",
        "                    preds = yhat.max(1)[1]\n",
        "                    predictions.extend(preds.int().tolist())\n",
        "                    labels.extend(label.int().tolist())\n",
        "                    torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # test accuracy.\n",
        "            accTest = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # output information.\n",
        "        if (0 == (epoch + 1) % _TRnnPerEpoch_):\n",
        "            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
        "            if (markTest):\n",
        "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
        "            print(strAcc)\n",
        "        # save the best model.\n",
        "        if (accList[-1] > max(accList[0:-1])):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_TwinRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch >= _TRnnJudEpoch_) and (accList[-1] < min(accList[-1-_TRnnJudEpoch_:-1])):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n",
        "    print('[INFO] <TwinRNNTrain> Finish training TwinRNN model. (Best model: ' + tempPath + '/model_TwinRNN.pth)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def TwinRNNTest(model, dTest, lTest, batchsize=64):\n",
        "    '''\n",
        "    Test the TwinRNN model.\n",
        "    :param model: deep learning model.\n",
        "    :param dTest: test data.\n",
        "    :param lTest: test label.\n",
        "    :param batchsize: number of samples in a batch\n",
        "    :return: predictions - predicted labels. [[0], [1], ...]\n",
        "             accuracy - the total test accuracy. numeric\n",
        "    '''\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "\n",
        "    # batch size processing.\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # load the model of recurrent neural network.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            # data conversion.\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            # forward propagation.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "\n",
        "    return predictions, accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #demoDiffRNN()\n",
        "    #demoCommitMsg()\n",
        "    #demoPatch()\n",
        "    demoTwin()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[INFO] <ReadData> Load 38041 raw data from ./drive/My Drive/Colab Notebooks//temp//data.npy.\n",
            "[INFO] <GetDiffProps> Load 38041 diff property data from ./drive/My Drive/Colab Notebooks//temp//props.npy.\n",
            "[INFO] <AbstractTokens> Abstract the tokens of identifiers with iType 1 (VARn/FUNCn).\n",
            "[INFO] <AbstractTokens> Abstract the tokens of literals, and comments with iType 1 (LITERAL/n/COMMENT).\n",
            "[INFO] <GetDiffVocab> There are 35575 diff vocabulary tokens. (except '<pad>')\n",
            "[INFO] <GetDiffVocab> The max diff length is 2706522 tokens. (hyperparameter: _DiffMaxLen_ = 600)\n",
            "[INFO] <GetDiffDict> Create dictionary for 35576 diff vocabulary tokens. (with '<pad>')\n",
            "[INFO] <GetDiffEmbed> Create pre-trained embedding weights with 35576 * 128 matrix.\n",
            "[INFO] <DivideBeforeAfter> Divide diff code into BEFORE-version and AFTER-version code.\n",
            "[INFO] <DivideBeforeAfter> The max length in BEFORE/AFTER-version code is 1938015 tokens. (hyperparameter: _TwinMaxLen_ = 800)\n",
            "[INFO] <GetTwinMapping> Create 38041 feature data with 800 * 4 matrix.\n",
            "[INFO] <GetTwinMapping> Create 38041 labels with 1 * 1 matrix.\n",
            "[INFO] <UpdateTwinTokenTypes> Update 38041 feature data with 800 * 12 matrix.\n",
            "[INFO] <GetCommitMsg> Load 38041 commit messages from ./drive/My Drive/Colab Notebooks//temp//msgs.npy.\n",
            "[INFO] <GetMsgVocab> There are 80830 commit message vocabulary tokens. (except '<pad>')\n",
            "[INFO] <GetMsgVocab> The max msg length is 1434 tokens. (hyperparameter: _MsgMaxLen_ = 200)\n",
            "[INFO] <GetMsgDict> Create dictionary for 80831 msg vocabulary tokens. (with '<pad>')\n",
            "[INFO] <GetMsgEmbed> Create pre-trained embedding weights with 80831 * 128 matrix.\n",
            "[INFO] <GetMsgMapping> Create 38041 feature data with 1 * 200 vector.\n",
            "[INFO] <GetMsgMapping> Create 38041 labels with 1 * 1 matrix.\n",
            "[INFO] <CombineTwinMsgs> Combine the twin props with the commit messages.\n",
            "[INFO] <SplitData> Split data into 30433 REST dataset and 7608 TEST dataset. (Total: 38041, Rate: 20%)\n",
            "[INFO] <demoTwin> Get 30433 TRAIN data, 7608 TEST data. (Total: 38041)\n",
            "[INFO] <TwinRNNTrain> ModelType: TwinRNN.\n",
            "[INFO] <TwinRNNTrain> Code Part: EmbedDim: 128, MaxLen: 800, HidNodes: 32, HidLayers: 1.\n",
            "[INFO] <TwinRNNTrain> Msg  Part: EmbedDim: 128, MaxLen: 200, HidNodes: 32, HidLayers: 1.\n",
            "[INFO] <TwinRNNTrain> BatchSize: 256, LearningRate: 0.0005, MaxEpoch: 1000, PerEpoch: 1, JudEpoch: 10.\n",
            "[Epoch 001] loss: 0.656, train acc: 63.786%, valid acc: 70.373%, test acc: 70.373%.\n",
            "[Epoch 002] loss: 0.552, train acc: 76.447%, valid acc: 77.392%, test acc: 77.392%.\n",
            "[Epoch 003] loss: 0.492, train acc: 82.466%, valid acc: 81.059%, test acc: 81.059%.\n",
            "[Epoch 004] loss: 0.463, train acc: 85.703%, valid acc: 82.242%, test acc: 82.242%.\n",
            "[Epoch 005] loss: 0.443, train acc: 87.701%, valid acc: 83.070%, test acc: 83.070%.\n",
            "[Epoch 006] loss: 0.428, train acc: 89.193%, valid acc: 82.676%, test acc: 82.676%.\n",
            "[Epoch 007] loss: 0.419, train acc: 90.142%, valid acc: 81.217%, test acc: 81.217%.\n",
            "[Epoch 008] loss: 0.413, train acc: 90.530%, valid acc: 80.783%, test acc: 80.783%.\n",
            "[Epoch 009] loss: 0.406, train acc: 91.112%, valid acc: 82.978%, test acc: 82.978%.\n",
            "[Epoch 010] loss: 0.398, train acc: 91.979%, valid acc: 83.570%, test acc: 83.570%.\n",
            "[Epoch 011] loss: 0.39, train acc: 92.807%, valid acc: 83.123%, test acc: 83.123%.\n",
            "[Epoch 012] loss: 0.385, train acc: 93.313%, valid acc: 82.282%, test acc: 82.282%.\n",
            "[Epoch 013] loss: 0.383, train acc: 93.376%, valid acc: 82.164%, test acc: 82.164%.\n",
            "[Epoch 014] loss: 0.384, train acc: 93.188%, valid acc: 81.401%, test acc: 81.401%.\n",
            "[Epoch 015] loss: 0.38, train acc: 93.668%, valid acc: 82.716%, test acc: 82.716%.\n",
            "[Epoch 016] loss: 0.377, train acc: 93.993%, valid acc: 83.176%, test acc: 83.176%.\n",
            "[Epoch 017] loss: 0.375, train acc: 94.204%, valid acc: 83.162%, test acc: 83.162%.\n",
            "[Epoch 018] loss: 0.374, train acc: 94.361%, valid acc: 83.268%, test acc: 83.268%.\n",
            "[Epoch 019] loss: 0.373, train acc: 94.375%, valid acc: 83.189%, test acc: 83.189%.\n",
            "[Epoch 020] loss: 0.375, train acc: 94.184%, valid acc: 83.294%, test acc: 83.294%.\n",
            "[Epoch 021] loss: 0.372, train acc: 94.470%, valid acc: 82.558%, test acc: 82.558%.\n",
            "[Epoch 022] loss: 0.371, train acc: 94.565%, valid acc: 83.044%, test acc: 83.044%.\n",
            "[Epoch 023] loss: 0.371, train acc: 94.634%, valid acc: 82.755%, test acc: 82.755%.\n",
            "[Epoch 024] loss: 0.37, train acc: 94.670%, valid acc: 83.189%, test acc: 83.189%.\n",
            "[Epoch 025] loss: 0.37, train acc: 94.693%, valid acc: 83.412%, test acc: 83.412%.\n",
            "[Epoch 026] loss: 0.369, train acc: 94.782%, valid acc: 83.530%, test acc: 83.530%.\n",
            "[Epoch 027] loss: 0.369, train acc: 94.779%, valid acc: 83.478%, test acc: 83.478%.\n",
            "[Epoch 028] loss: 0.37, train acc: 94.723%, valid acc: 83.268%, test acc: 83.268%.\n",
            "[Epoch 029] loss: 0.369, train acc: 94.812%, valid acc: 83.504%, test acc: 83.504%.\n",
            "[Epoch 030] loss: 0.368, train acc: 94.884%, valid acc: 83.320%, test acc: 83.320%.\n",
            "[Epoch 031] loss: 0.368, train acc: 94.871%, valid acc: 83.307%, test acc: 83.307%.\n",
            "[Epoch 032] loss: 0.368, train acc: 94.887%, valid acc: 83.136%, test acc: 83.136%.\n",
            "[Epoch 033] loss: 0.368, train acc: 94.923%, valid acc: 83.215%, test acc: 83.215%.\n",
            "[Epoch 034] loss: 0.368, train acc: 94.890%, valid acc: 82.860%, test acc: 82.860%.\n",
            "[INFO] <TwinRNNTrain> Finish training TwinRNN model. (Best model: ./drive/My Drive/Colab Notebooks//temp//model_TwinRNN.pth)\n",
            "       -------------------------------------------\n",
            "       method           :  TwinRNN\n",
            "       accuracy  (ACC)  :  83.570%\n",
            "       precision (P)    :  75.719%\n",
            "       recall    (R)    :  73.661%\n",
            "       F1 score  (F1)   :  0.747\n",
            "       fall-out  (FPR)  :  11.575%\n",
            "       miss rate (FNR)  :  26.339%\n",
            "       confusion matrix :      (actual)\n",
            "                           Neg         Pos\n",
            "       (predicted) Neg     4515 (TN)   659  (FN)\n",
            "                   Pos     591  (FP)   1843 (TP)\n",
            "       -------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}